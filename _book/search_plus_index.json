{"./":{"url":"./","title":"0. Introduction","keywords":"","body":"BrainPy Introduction In this chapter, we will briefly introduce how to implement computational neuroscience models with BrainPy. For more detailed documents and tutorials, please check our Github repository BrainPy and BrainModels. BrainPy is a Python platform for computational neuroscience and brain-inspired computation. To model with BrainPy, users should follow 3 steps: 1) Define Python classes for neuron and synapse models. BrainPy provides base classes for different kinds of models, users only need to inherit from those base classes, and define specific methods to tell BrainPy what operations they want the models to take during the simulation. In this process, BrainPy will assist users in the numerical integration of differential equations (ODE, SDE, etc.), adaptation of various backends (Numpy, PyTorch, etc.), and other functions to simplify code logic. 2) Instantiate Python classes as objects of neuron group and synapse connection groups, pass the instantiated objects to BrainPy class Network, and call method run to simulate the network. 3) Call BrainPy modules like the measure module and the visualize module to display the simulation results. With this overall concept of BrainPy, we will go into more detail about implementations in the following sections. In neural systems, neurons are connected by synapses to build networks, so we will introduce neuron models, synapse models, and network models in order. "},"neurons.html":{"url":"neurons.html","title":"1. Neuron models","keywords":"","body":"1. Neuron models Neuron models can be classified into three types from complex to simple: biophysical models, reduced models and firing rate models. 1.1 Biological Background 1.2 Biophysical models 1.3 Reduced models 1.4 Firing rate models "},"neurons/biological_background.html":{"url":"neurons/biological_background.html","title":"1.1 Biological background","keywords":"","body":"1.1 Biological backgrounds As the basic unit of neural systems, neurons maintain mystique to researchers for a long while. In recent centuries, however, along with the development of experimental techniques, researchers have painted a general figure of those little things working ceaselessly in our neural system. To achieve our final goal of modeling neurons with computational neuroscience methods, we may start with a patch of real neuron membrane. Fig. 1-1 神经元细胞膜示意图 | what-when-how.com The figure above is a general diagram of neuron membrane with phospholipid bilayer and ion channels. The membrane divides the ions and fluid into intracellular and extracellular, partially prevent them from exchanging, thus generates membrane potential---- the difference in electric potential across the membrane. An ion in the fluid is subjected to two forces. The force of diffusion is caused by the ion concentration difference across the membrane, while the force of electric field is caused by the electric potential difference. When these two forces reach balance, the total forces on ions are 0, and each type of ion meets an equilibrium potential, while the neuron holds a membrane potential lower than 0. This membrane potential integrated by all those ion equilibrium potentials is the resting potential, and the neuron is, in a so-called resting state. If the neuron is not disturbed, it will just come to the balanced resting state, and rest. However, our neural system receives countless inputs every millisecond, from external inputs to recurrent inputs, from specific stimulus inputs to non-specific background inputs. Receiving all these inputs, neurons generate action potentials (or spikes) to transfer and process information all across the neural system. Fig. 1-2 动作电位 | Wikipedia Passing through the ion channels shown in Fig.1-1, ions on both sides of the hydrophobic phospholipid bilayer are exchanged. Due to changes in the environment caused by, for example, an external input, ion channels will switch between their open/close states, therefore allow/prohibit ion exchanges. During the switch, the ion concentrations (mainly Na+ and K+) change, induce a significant change on neuron's membrane potential: the membrane potential will raise to a peak value and then fall back in a short time period. Biologically, when such a series of potential changes happens, we say the neuron generates an action potential or a spike, or the neuron fires. An action potential can be mainly divided into three periods: depolarization, repolarization and refractory period. During the depolarization period, Na+ flow into the neuron and K+ flow out of the neuron, however the inflow of Na+ is faster, so the membrane potential raises from a low value VrestV_{rest}V​rest​​ to a value much higher called VthV_{th}V​th​​, then the outflow of K+ becomes faster than Na+, and the membrane potential is reset to a value lower than resting potential during the repolarization period. After that, because of the relatively lower membrane potential, the neuron is unlikely to generate another spike immediately, until the refractory period passes. A single action potential is complex enough, but in our neural system, one single neuron can generate several action potentials in less than a second. How, exactly, do the neurons fire? Different kinds of neurons may spike when facing different inputs, and the pattern of their spiking can be classified into several firing patterns, some of which are shown in the following figure. Figure 1-3 Some firing patterns Those firing patterns, together with the shape of action potentials, are what computational neuroscience wants to model at the cellular level. "},"neurons/biophysical_models.html":{"url":"neurons/biophysical_models.html","title":"1.2 Biophysical models","keywords":"","body":"1.2 Biophysical models 1.2.1 Hodgkin-Huxley model Hodgkin and Huxley (1952) recorded the generation of action potential on squid giant axons with voltage clamp technique, and proposed the canonical neuron model called Hodgin-Huxley model (HH model). In last section we have introduced a general template for neuron membrane. Computational neuroscientists always model neuron membrane as equivalent circuit like the following figure. Fig. 1-4 Equivalent circuit diagram | NeuroDynamics The equivalent circuit diagram of Fig.1-1 is shown in Fig. 1-4, in which the patch of neuron membrane is converted into electric components. In Fig.1-4, the capacitance CCC refers to the hydrophobic phospholipid bilayer with low conductance, and current III refers to the external stimulus. As Na+ ion channels and K+ ion channels are important in the generation of action potentials, these two ion channels are modeled as the two variable resistances RNaR_{Na}R​Na​​ and RKR_KR​K​​ in parallel on the right side of the circuit diagram, and the resistance RRR refers to all the non-specific ion channels on the membrane. The batteries ENaE_{Na}E​Na​​, EKE_KE​K​​ and ELE_LE​L​​ refer to the electric potential differences caused by the concentration differences of corresponding ions. Consider the Kirchhoff’s first law, that is, for any node in an electrical circuit, the sum of currents flowing into that node is equal to the sum of currents flowing out of that node, Fig. 1-4 can be modeled as differential equations: CdVdt=−(g¯Nam3h(V−ENa)+g¯Kn4(V−EK)+gleak(V−Eleak))+I(t)\r C \\frac{dV}{dt} = -(\\bar{g}_{Na} m^3 h (V - E_{Na}) + \\bar{g}_K n^4(V - E_K) + g_{leak}(V - E_{leak})) + I(t)\r C​dt​​dV​​=−(​g​¯​​​Na​​m​3​​h(V−E​Na​​)+​g​¯​​​K​​n​4​​(V−E​K​​)+g​leak​​(V−E​leak​​))+I(t) dxdt=αx(1−x)−βx,x∈{Na,K,leak}\r \\frac{dx}{dt} = \\alpha_x(1-x) - \\beta_x , x \\in \\{ Na, K, leak \\}\r ​dt​​dx​​=α​x​​(1−x)−β​x​​,x∈{Na,K,leak} That is the HH model. Note that in the first equation above, the first three terms on the right hand are the current go through Na+ ion channels, K+ ion channels and other non-specific ion channels, respectively, while I(t)I(t)I(t) is an external input. On the left hand, CdVdt=dQdt=IC\\frac{dV}{dt} = \\frac{dQ}{dt} = IC​dt​​dV​​=​dt​​dQ​​=I is the current go through the capacitance. In the computing of ion channel currents, other than the Ohm's law I=U/R=gUI = U/R = gUI=U/R=gU, HH model introduces three gating variables m, n and h to control the open/close state of ion channels. To be precise, variables m and h control the state of Na+ ion channel, variable n controls the state of K+ ion channel, and the real conductance of an ion channel is the product of maximal conductance g¯\\bar{g}​g​¯​​ and the state of gating variables. Gating variables' dynamics can be expressed in a Markov-like form, in which αx\\alpha_xα​x​​ refers to the activation rate of gating variable x, and βx\\beta_xβ​x​​ refers to the de-activation rate of x. The expressions of αx\\alpha_xα​x​​ and βx\\beta_xβ​x​​ (as shown in equations below) are fitted by experimental data. αm(V)=0.1(V+40)1−exp(−(V+40)10)\r \\alpha_m(V) = \\frac{0.1(V+40)}{1 - exp(\\frac{-(V+40)}{10})}\r α​m​​(V)=​1−exp(​10​​−(V+40)​​)​​0.1(V+40)​​ βm(V)=4.0exp(−(V+65)18)\r \\beta_m(V) = 4.0 exp(\\frac{-(V+65)}{18})\r β​m​​(V)=4.0exp(​18​​−(V+65)​​) αh(V)=0.07exp(−(V+65)20)\r \\alpha_h(V) = 0.07 exp(\\frac{-(V+65)}{20})\r α​h​​(V)=0.07exp(​20​​−(V+65)​​) βh(V)=11+exp(−(V+35)10)\r \\beta_h(V) = \\frac{1}{1 + exp(\\frac{-(V + 35)}{10})}\r β​h​​(V)=​1+exp(​10​​−(V+35)​​)​​1​​ αn(V)=0.01(V+55)1−exp(−(V+55)10)\r \\alpha_n(V) = \\frac{0.01(V+55)}{1 - exp(\\frac{-(V+55)}{10})}\r α​n​​(V)=​1−exp(​10​​−(V+55)​​)​​0.01(V+55)​​ βn(V)=0.125exp(−(V+65)80)\r \\beta_n(V) = 0.125 exp(\\frac{-(V+65)}{80})\r β​n​​(V)=0.125exp(​80​​−(V+65)​​) Run codes in our github repository: https://github.com/PKU-NIP-Lab/BrainModels The V-t plot of HH model simulated by BrainPy is shown below. The three periods, depolarization, repolarization and refractory period of a real action potential can be seen in the V-t plot. In addition, during the depolarization period, the membrane integrates external inputs slowly at first, and increases rapidly once it grows beyond some point, which also reproduces the \"shape\" of action potentials. "},"neurons/reduced_models.html":{"url":"neurons/reduced_models.html","title":"1.3 Reduced models","keywords":"","body":"1.3 Reduced models Inspired by biophysical experiments, Hodgkin-Huxley model is precise but costly. Researchers proposed the reduced models to reduce the consumption on computing resources and running time in simulation. These models are simple and easy to compute, while they can still reproduce the main pattern of neuron behaviors. Although their representation capabilities are not as good as biophysical models, such a loss of accuracy is sometimes acceptable considering their simplicity. 1.3.1 Leaky Integrate-and-Fire model The most typical reduced model is the Leaky Integrate-and-Fire model (LIF model) presented by Lapicque (1907). LIF model is a combination of integrate process represented by differential equation and spike process represented by conditional judgment: τdVdt=−(V−Vrest)+RI(t)\r \\tau\\frac{dV}{dt} = - (V - V_{rest}) + R I(t)\r τ​dt​​dV​​=−(V−V​rest​​)+RI(t) If V>VthV > V_{th}V>V​th​​, neuron fires, V←Vreset\r V \\gets V_{reset}\r V←V​reset​​ τ=RC\\tau = RCτ=RC is the time constant of LIF model, the larger τ\\tauτ is, the slower model dynamics is. The equation shown above is corresponding to a simpler equivalent circuit than HH model, for it does not model the Na+ and K+ ion channels any more. Actually, in LIF model, only the consistence RRR, capacitance CCC, battery EEE and external input III is modeled. Fig1-4 Equivalent circuit of LIF model Compared with HH model, LIF model does not model the shape of action potentials, which means, the membrane potential does not burst before a spike. Also, the refractory period is overlooked in the original model, and in order to generate it, another conditional judgment must be added: If t−tlastspike=refractoryperiod\r t-t_{last spike}t−t​lastspike​​=refractoryperiod then neuron is in refractory period, membrane potential VVV will not be updated. 1.3.2 Quadratic Integrate-and-Fire model To pursue higher representation capability, Latham et al. (2000) proposed Quadratic Integrate-and-Fire model (QuaIF model), in which they add a second order term in differential equation so the neurons can generate spike better. τdVdt=a0(V−Vrest)(V−Vc)+RI(t)\r \\tau\\frac{d V}{d t}=a_0(V-V_{rest})(V-V_c) + RI(t)\r τ​dt​​dV​​=a​0​​(V−V​rest​​)(V−V​c​​)+RI(t) In the equation above, a0a_0a​0​​ is a parameter controls the slope of membrane potential before a spike, and VcV_cV​c​​ is the critical potential for action potential initialization. Below VCV_CV​C​​, membrane potential VVV increases slowly, once it grows beyond VCV_CV​C​​, VVV turns to rapid increase. 1.3.3 Exponential Integrate-and-Fire model Exponential Integrate-and-Fire model (ExpIF model) (Fourcaud-Trocme et al., 2003) is more expressive than QuaIF model. With the exponential term added to the right hand of differential equation, the dynamics of ExpIF model can now generates a more realistic action potential. τdVdt=−(V−Vrest)+ΔTeV−VTΔT+RI(t)\r \\tau \\frac{dV}{dt} = - (V - V_{rest}) + \\Delta_T e^{\\frac{V - V_T}{\\Delta_T}} + R I(t)\r τ​dt​​dV​​=−(V−V​rest​​)+Δ​T​​e​​Δ​T​​​​V−V​T​​​​​​+RI(t) In the exponential term, VTV_TV​T​​ is the critical potential of generating action potential, below which VVV increases slowly and above which rapidly. ΔT\\Delta_TΔ​T​​ is the slope of action potentials in ExpIF model, and when ΔT→0\\Delta_T\\to 0Δ​T​​→0, the shape of spikes in ExpIF model will be equivalent to the LIF model with Vth=VTV_{th} = V_TV​th​​=V​T​​(Fourcaud-Trocme et al., 2003) . 1.3.4 Adaptive Exponential Integrate-and-Fire model While facing a constant stimulus, the response generated by a single neuron will sometimes decreases over time, this phenomenon is called adaptation in biology. To reproduce the adaptation behavior of neurons, researchers add a weight variable www to existing integrate-and-fire models like LIF, QuaIF and ExpIF models. Here we introduce a typical one: Adaptive Exponential Integrate-and-Fire model (AdExIF model) (Gerstner et al., 2014). τmdVdt=−(V−Vrest)+ΔTeV−VTΔT−Rw+RI(t)\r \\tau_m \\frac{dV}{dt} = - (V - V_{rest}) + \\Delta_T e^{\\frac{V - V_T}{\\Delta_T}} - R w + R I(t)\r τ​m​​​dt​​dV​​=−(V−V​rest​​)+Δ​T​​e​​Δ​T​​​​V−V​T​​​​​​−Rw+RI(t) τwdwdt=a(V−Vrest)−w+bτw∑δ(t−tf))\r \\tau_w \\frac{dw}{dt} = a(V - V_{rest})- w + b \\tau_w \\sum \\delta(t - t^f))\r τ​w​​​dt​​dw​​=a(V−V​rest​​)−w+bτ​w​​∑δ(t−t​f​​)) The first differential equation of AdExIF model, as the model's name shows, is similar to ExpIF model we introduced above, except for the term of adaptation, which is shown as −Rw-Rw−Rw in the equation. The weight term www is regulated by the second differential equation. aaa describes the sensitivity of the recovery variable www to the sub-threshold fluctuations of VVV, and bbb is the increment value of www generated by a spike, and www will also decay over time. Give AdExIF neuron a constant input, after several spikes, the value of www will increase to a high value, which slows down the rising speed of VVV, thus reduces the neuron's firing rate. 1.3.5 Hindmarsh-Rose model To simulate the bursting spike pattern in neurons (i.e., continuously firing in a short time period), Hindmarsh and Rose (1984) proposed Hindmarsh-Rose model, import a third model variable zzz as slow variable to control the bursting of neuron. dVdt=y−aV3+bV2−z+I\r \\frac{d V}{d t} = y - a V^3 + b V^2 - z + I\r ​dt​​dV​​=y−aV​3​​+bV​2​​−z+I dydt=c−dV2−y\r \\frac{d y}{d t} = c - d V^2 - y\r ​dt​​dy​​=c−dV​2​​−y dzdt=r(s(V−Vrest)−z)\r \\frac{d z}{d t} = r (s (V - V_{rest}) - z)\r ​dt​​dz​​=r(s(V−V​rest​​)−z) The VVV variable refers to membrane potential, and yyy, zzz are two gating variables. The parameter bbb in dVdt\\frac{dV}{dt}​dt​​dV​​ equation allows the model to switch between spiking and bursting states, and controls the spiking frequency. rrr controls slow variable zzz's variation speed, affects the number of spikes per burst when bursting, and governs the spiking frequency together with bbb. The parameter sss governs adaptation, and other parameters are fitted by firing patterns. In the variable-t plot painted below, we may see that the slow variable zzz changes much slower than VVV and yyy. Also, VVV and yyy are changing periodically during the simulation. With the theoretical analysis module analysis of BrainPy, we may explain the existence of this periodicity through theoretical analysis. In Hindmarsh-Rose model, the trajectory of VVV and yyy approaches a limit cycle in phase plane, therefore their values change periodically along the limit cycle. 1.3.6 Generalized Integrate-and-Fire model Generalized Integrate-and-Fire model (GIF model) (Mihalaş et al., 2009) integrates several firing patterns in one model. With 4 model variables, it can generate more than 20 types of firing patterns, and is able to alternate between patterns by fitting parameters. dIjdt=−kjIj,j=1,2\r \\frac{d I_j}{d t} = - k_j I_j, j = {1, 2}\r ​dt​​dI​j​​​​=−k​j​​I​j​​,j=1,2 τdVdt=(−(V−Vrest)+R∑jIj+RI)\r \\tau \\frac{d V}{d t} = ( - (V - V_{rest}) + R\\sum_{j}I_j + RI)\r τ​dt​​dV​​=(−(V−V​rest​​)+R∑​j​​I​j​​+RI) dVthdt=a(V−Vrest)−b(Vth−Vth∞)\r \\frac{d V_{th}}{d t} = a(V - V_{rest}) - b(V_{th} - V_{th\\infty})\r ​dt​​dV​th​​​​=a(V−V​rest​​)−b(V​th​​−V​th∞​​) When VVV meets VthV_{th}V​th​​, Generalized IF neuron fire: Ij←RjIj+Aj\r I_j \\leftarrow R_j I_j + A_j\r I​j​​←R​j​​I​j​​+A​j​​ V←Vreset\r V \\leftarrow V_{reset}\r V←V​reset​​ Vth←max(Vthreset,Vth)\r V_{th} \\leftarrow max(V_{th_{reset}}, V_{th})\r V​th​​←max(V​th​reset​​​​,V​th​​) In the dVdt\\frac{dV}{dt} ​dt​​dV​​ differential equation, just like all the integrate-and-fire models, τ\\tauτ is time constant, VVV is membrane potential, VrestV_{rest}V​rest​​ is resting potential, RRR is conductance, and III is external input. However, in GIF model, variable amounts of internal currents are added to the equation, shown as the ∑jIj\\sum_j I_j∑​j​​I​j​​ term. Each Ij I_j I​j​​ is an internal current in the neuron, with a decay rate of kjk_jk​j​​. RjR_jR​j​​ and AjA_jA​j​​ are free parameters, RjR_jR​j​​ describes the dependence of IjI_jI​j​​ reset value on the value of IjI_jI​j​​ before spike, and AjA_jA​j​​ is a constant value added to the reset value after spike. The variable threshold potential VthV_{th}V​th​​ is regulated by two parameters: aaa describes the dependence of VthV_{th}V​th​​ on the membrane potential VVV, and bbb is the rate VthV_{th}V​th​​ approaches the infinite value of threshold Vth∞V_{th_{\\infty}}V​th​∞​​​​. VthresetV_{th_{reset}}V​th​reset​​​​ is the reset value of threshold potential when neuron fires. "},"neurons/firing_rate_models.html":{"url":"neurons/firing_rate_models.html","title":"1.4 Firing rate models","keywords":"","body":"1.4 Firing Rate models Firing Rate models are simpler than reduced models. In these models, each compute unit represents a neuron group, the membrane potential variable VVV in single neuron models is replaced by firing rate variable aaa (or rrr or ν\\nuν). Here we introduce a canonical firing rate unit. 1.4.1 Firing Rate Unit Wilson and Cowan (1972) proposed this unit to represent the activities in excitatory and inhibitory cortical neuron columns. Each element of variables aea_ea​e​​ and aia_ia​i​​ refers to the average activity of a neuron column group contains multiple neurons. τedae(t)dt=−ae(t)+(ke−re∗ae(t))∗S(c1ae(t)−c2ai(t)+Iexte(t))\r \\tau_e \\frac{d a_e(t)}{d t} = - a_e(t) + (k_e - r_e * a_e(t)) * \\mathcal{S}(c_1 a_e(t) - c_2 a_i(t) + I_{ext_e}(t))\r τ​e​​​dt​​da​e​​(t)​​=−a​e​​(t)+(k​e​​−r​e​​∗a​e​​(t))∗S(c​1​​a​e​​(t)−c​2​​a​i​​(t)+I​ext​e​​​​(t)) τidai(t)dt=−ai(t)+(ki−ri∗ai(t))∗S(c3ae(t)−c4ai(t)+Iexti(t))\r \\tau_i \\frac{d a_i(t)}{d t} = - a_i(t) + (k_i - r_i * a_i(t)) * \\mathcal{S}(c_3 a_e(t) - c_4 a_i(t) + I_{ext_i}(t))\r τ​i​​​dt​​da​i​​(t)​​=−a​i​​(t)+(k​i​​−r​i​​∗a​i​​(t))∗S(c​3​​a​e​​(t)−c​4​​a​i​​(t)+I​ext​i​​​​(t)) S(x)=11+exp(−a(x−θ))−11+exp(aθ)\r \\mathcal{S}(x) = \\frac{1}{1 + exp(- a(x - \\theta))} - \\frac{1}{1 + exp(a\\theta)}\r S(x)=​1+exp(−a(x−θ))​​1​​−​1+exp(aθ)​​1​​ The subscript x∈{e,i}x\\in\\{e, i\\}x∈{e,i} points out whether this parameter or variable corresponds to excitatory or inhibitory neuron group. In the differential equations, τx\\tau_xτ​x​​ refers to the time constant of neuron columns, parameters kxk_xk​x​​ and rxr_xr​x​​ control the refractory periods, axa_xa​x​​ and θx\\theta_xθ​x​​ refer to the slope factors and phase parameters of sigmoid functions, and external inputs IextxI_{ext_{x}}I​ext​x​​​​ are given separately to excitatory and inhibitory neuron groups. "},"networks.html":{"url":"networks.html","title":"3. Network models","keywords":"","body":"3. Network models With the neuron and synapse models we have realized with BrainPy, users can now build networks of there own. In this section, we will introduce two main types of network models as examples: 1) spiking neural networks that model and compute each neuron or synapse separately; 2) firing rate networks that simplify neuron groups in the network as firing rate units and compute each neuron group as one unit. 3.1 Spiking Neural Networks 3.2 Firing Rate Networks "},"networks/spiking_neural_networks.html":{"url":"networks/spiking_neural_networks.html","title":"3.1 Spiking neural networks","keywords":"","body":"3.1 Spiking Neural Network 3.1.1 E/I balanced network In 1990s, biologists found in experiments that neuron activities in brain cortex show a temporal irregular spiking pattern. This pattern exists widely in brain areas, but researchers knew few about its mechanism or function. Vreeswijk and Sompolinsky (1996) proposed E/I balanced network to explain this irregular spiking pattern. The feature of this network is the strong, random and sparse synapse connections between neurons. Because of this feature and corresponding parameter settings, each neuron in the network will receive great excitatory and inhibitory input from within the network. However, these two types of inputs will cancel each other, and maintain the total internal input at a relatively small order of magnitude, which is only enough to generate action potentials. The randomness and noise in E/I balanced network give each neuron in the network an internal input which varies with time and space at the order of threshold potential. Therefore, the firing of neurons also has randomness, ensures that E/I balanced network can generate temporal irregular firing pattern spontaneously. Fig.3-1 Structure of E/I balanced network | Vreeswijk and Sompolinsky, 1996 Vreeswijk and Sompolinsky also suggested a possible function of this irregular firing pattern: E/I balanced network can respond to the changes of external stimulus quickly. As shown in Fig. 3-3, when there is no external input, the distribution of neurons’ membrane potentials in E/I balanced network follows a relatively uniform random distribution between resting potential V0V_0V​0​​and threshold potential θ\\thetaθ. Fig.3-2 Distribution of neuron membrane potentials in E/I balanced network | Tian et al., 2020 When we give the network a small constant external stimulus, those neurons whose membrane potentials fall near the threshold potential will soon meet the threshold, therefore spike rapidly. On the network scale, the firing rate of the network can adjust rapidly once the input changes. Simulation suggests that the delay of network response to input and the delay of synapses have the same time scale, and both are significantly smaller than the delay of a single neuron from resting potential to generating a spike. So E/I balanced network may provide a fast response mechanism for neural networks. Fig. 3-1 shows the structure of E/I balanced network: 1) Neurons: Neurons are realized with LIF neuron model. The neurons can be divided into excitatory neurons and inhibitory neurons, the ratio of the two types of neurons is NEN_EN​E​​: NIN_IN​I​​ = 4:1. 2) Synapses: Synapses are realized with exponential synapse model. 4 groups of synapse connections are generated between the two groups of neurons, that is, excitatory-excitatory connection (E2E conn), excitatory-inhibitory connection (E2I conn), inhibitory-excitatory connection (I2E conn) and inhibitory-inhibitory connection (I2I conn). For excitatory or inhibitory synapse connections, we define synapse weights with different signal. 3) Inputs: All neurons in the network receive a constant external input current. See above section 1 and 2 for definition of LIF neuron and exponential synapse. After simulation, we visualize the raster plot and firing rate-t plot of E/I balanced network. the network firing rate changes from strong synchronization to irregular fluctuation. Fig.3-3 E/I balanced net raster plot 3.1.2 Decision Making Network The modeling of computational neuroscience networks can correspond to specific physiological tasks. For example, in the visual motion discrimination task (Roitman and Shadlen, 2002), rhesus watch a video in which random dots move towards left or right with definite coherence. Rhesus are required to choose the direction that most dots move to and give their answer by saccade. At the meantime, researchers record the activity of their LIP neurons by implanted electrode. Fig.3-4 Experimental Diagram Wang (2002) proposed a decision making network to model the activity of rhesus LIP neurons during decision making period in the visual motion discrimination task. As shown in Fig. 3-5, this network is based on E/I balanced network. The ratio of excitatory neurons and inhibitory neurons is NE:NI=4:1N_E:N_I = 4:1N​E​​:N​I​​=4:1, and parameters are adjusted to maintain the balanced state. To accomplish the decision making task, among the excitatory neuron group, two selective subgroup A and B are chosen, both with a size of NA=NB=0.15NEN_A = N_B = 0.15N_EN​A​​=N​B​​=0.15N​E​​. These two subgroups are marked as A and B in Fig. 3-5, and we call other excitatory neurons as non-selective neurons, Nnon=(1−2∗0.15)NEN_{non} = (1-2*0.15)N_EN​non​​=(1−2∗0.15)N​E​​. Fig.3-5 structure of decision makingnetwork As it is in E/I balanced network, 4 groups of synapses ---- E2E connection, E2I connection, I2E connection and I2I connection ---- are built in decision making network. Excitatory connections are realized with AMPA synapse, inhibitory connections are realized with GABAa synapse. Decision making network needs to make a decision among the two choice, i.e., among the two subgroups A and B in this task. To achieve this, network must discriminate between these two groups. Excitatory neurons in the same subgroup should self-activate, and inhibit neurons in another selective subgroup. Therefore, E2E connections are structured in the network. As shown in Sheet 3-1, w+>1>w−w+ > 1 > w-w+>1>w−. In this way, a relative activation is established within the subgroups by stronger excitatory synapse connections, and relative inhibition is established between two subgroups or between selective and non-selective subgroups by weaker excitatory synapse connections. Sheet 3-1 Weight of synapse connections between E-neurons We give two types of external inputs to the decision making network: 1) Background inputs from other brain areas without specific meaning. Represented as high frequency Poisson input mediated by AMPA synapse. 2) Stimulus inputs from outside the brain, which are given only to the two selective subgroup A and B. Represented as lower frequency Poisson input mediated by AMPA synapse. The frequency of Poisson input given to A and B subgroup have a certain difference, simulate the difference in the number of dots moving to left and right in physiological experiments, induce the network to make a decision among these two subgroups. ρA=ρB=μ0/100\r \\rho_A = \\rho_B = \\mu_0/100\r ρ​A​​=ρ​B​​=μ​0​​/100 μA=μ0+ρA∗c\r \\mu_A = \\mu_0 + \\rho_A * c\r μ​A​​=μ​0​​+ρ​A​​∗c μB=μ0+ρB∗c\r \\mu_B = \\mu_0 + \\rho_B * c\r μ​B​​=μ​0​​+ρ​B​​∗c Every 50ms, the Poisson frequencies fxf_xf​x​​ change once, follows a Gaussian distribution defined by mean μx\\mu_xμ​x​​ and variance δ2\\delta^2δ​2​​. fA∼N(μA,δ2)\r f_A \\sim N(\\mu_A, \\delta^2)\r f​A​​∼N(μ​A​​,δ​2​​) fB∼N(μB,δ2)\r f_B \\sim N(\\mu_B, \\delta^2)\r f​B​​∼N(μ​B​​,δ​2​​) During the simulation, subgroup A receives a larger stimulus input than B, after a definite delay period, the activity of group A is significantly higher than group B, which means, the network chooses the right direction. Fig.3-6 decision making network "},"networks/rate_models.html":{"url":"networks/rate_models.html","title":"3.2 Firing rate networks","keywords":"","body":"3.2 Firing rate networks 3.2.1 Decision model In addition to spiking models, BrainPy can also implement Firing rate models. Let's first look at the implementation of a simplified version of the decision model. The model was simplified by the researcher (Wong & Wang, 2006)1 through a series of means such as mean field approach. In the end, there are only two variables, S1S_1S​1​​ and S2S_2S​2​​, which respectively represent the state of two neuron groups and correspond to two options. Fig. 3-1 Reduced decision model. (From Wong & Wang, 2006 1) The model is given by, dS1dt=−S1τ+(1−S1)γr1\r \\frac{dS_1} {dt} = -\\frac {S_1} \\tau + (1-S_1) \\gamma r_1\r ​dt​​dS​1​​​​=−​τ​​S​1​​​​+(1−S​1​​)γr​1​​ dS2dt=−S2τ+(1−S2)γr2\r \\frac{dS_2} {dt} = -\\frac {S_2} \\tau + (1-S_2) \\gamma r_2\r ​dt​​dS​2​​​​=−​τ​​S​2​​​​+(1−S​2​​)γr​2​​ where r1r_1 r​1​​ and r2r_2r​2​​ is the firing rate of two neuron groups, which is given by the input-output function, ri=f(Isyn,i)\r r_i = f(I_{syn, i})\r r​i​​=f(I​syn,i​​) f(I)=aI−b1−exp[−d(aI−b)]\r f(I)= \\frac {aI-b} {1- \\exp [-d(aI-b)]}\r f(I)=​1−exp[−d(aI−b)]​​aI−b​​ where Isyn,iI_{syn, i}I​syn,i​​ is given by the model structure (Fig. 3-1), Isyn,1=J11S1−J12S2+I0+I1\r I_{syn, 1} = J_{11} S_1 - J_{12} S_2 + I_0 + I_1\r I​syn,1​​=J​11​​S​1​​−J​12​​S​2​​+I​0​​+I​1​​ Isyn,2=J22S2−J21S1+I0+I2\r I_{syn, 2} = J_{22} S_2 - J_{21} S_1 + I_0 + I_2\r I​syn,2​​=J​22​​S​2​​−J​21​​S​1​​+I​0​​+I​2​​ where I0I_0I​0​​ is the background current, and the external inputs I1,I2I_1, I_2I​1​​,I​2​​ are determined by the total input strength μ0\\mu_0μ​0​​ and a coherence c′c'c​′​​. The higher the coherence, the more definite S1S_1S​1​​ is the correct answer, while the lower the coherence, the more random it is. The formula is as follows: I1=JA, extμ0(1+c′100%)\r I_1 = J_{\\text{A, ext}} \\mu_0 (1+\\frac {c'}{100\\%})\r I​1​​=J​A, ext​​μ​0​​(1+​100%​​c​′​​​​) I2=JA, extμ0(1−c′100%)\r I_2 = J_{\\text{A, ext}} \\mu_0 (1-\\frac {c'}{100\\%})\r I​2​​=J​A, ext​​μ​0​​(1−​100%​​c​′​​​​) The code implementation is as follows: we can create a neuron group class, and use S1S_1S​1​​ and S2S_2S​2​​ to store the two states of the neuron group. The dynamics of the model can be implemented by a derivative function for dynamics analysis. Then we can define a function to perform phase plane analysis. Let's first look at the case when there is no external input. At this time, μ0=0\\mu_0 = 0μ​0​​=0. phase_analyze(I=0., coh=0.) Output: plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.06176109215560733, s1=0.061761097890810475 is a stable node. Fixed point #2 at s2=0.029354239100062428, s1=0.18815448592736211 is a saddle node. Fixed point #3 at s2=0.0042468423702408655, s1=0.6303045696241589 is a stable node. Fixed point #4 at s2=0.6303045696241589, s1=0.004246842370235128 is a stable node. Fixed point #5 at s2=0.18815439944520335, s1=0.029354240536530615 is a saddle node. plot vector field ... It can be seen that it is very convenient to use BrainPy for dynamics analysis. The vector field and fixed point indicate which option will fall in the end under different initial values. Here, the x-axis is S2S_2S​2​​ which represents choice 2, and the y-axis is S1S_1S​1​​, which represents choice 1. As you can see, the upper-left fixed point represents choice 1, the lower-right fixed point represents choice 2, and the lower-left fixed point represents no choice. Now let's see which option will eventually fall under different initial values with different coherence, and we fix the external input strength to 30. Now let's look at the phase plane under different coherences when we fix the external input strength to 30. # coherence = 0% print(\"coherence = 0%\") phase_analyze(I=30., coh=0.) # coherence = 51.2% print(\"coherence = 51.2%\") phase_analyze(I=30., coh=0.512) # coherence = 100% print(\"coherence = 100%\") phase_analyze(I=30., coh=1.) coherence = 0% plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.6993504413889349, s1=0.011622049526766405 is a stable node. Fixed point #2 at s2=0.49867489858358865, s1=0.49867489858358865 is a saddle node. Fixed point #3 at s2=0.011622051540013889, s1=0.6993504355529329 is a stable node. plot vector field ... coherence = 51.2% plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.5673124813731691, s1=0.2864701069327971 is a saddle node. Fixed point #2 at s2=0.6655747347157656, s1=0.027835279565912054 is a stable node. Fixed point #3 at s2=0.005397687847426814, s1=0.7231453520305031 is a stable node. plot vector field ... coherence = 100% plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.0026865954387078755, s1=0.7410985604497689 is a stable node. plot vector field ... 3.2.2 CANN Let's see another example of firing rate model, a continuous attractor neural network (CANN)2. Fig. 3-2 demonstrates the structure of one-dimensional CANN. Fig. 3-2 Structure of CANN. (From Wu et al., 2008 2) We denote (x) as the parameter space site of the neuron group, and the dynamics of the total synaptic input of neuron group (x) u(x)u(x)u(x) is given by: τdu(x,t)dt=−u(x,t)+ρ∫dx′J(x,x′)r(x′,t)+Iext\r \\tau \\frac{du(x,t)}{dt} = -u(x,t) + \\rho \\int dx' J(x,x') r(x',t)+I_{ext}\r τ​dt​​du(x,t)​​=−u(x,t)+ρ∫dx​′​​J(x,x​′​​)r(x​′​​,t)+I​ext​​ Where r(x′,t)r(x', t)r(x​′​​,t) is the firing rate of the neuron group (x'), which is given by: r(x,t)=u(x,t)21+kρ∫dx′u(x′,t)2\r r(x,t) = \\frac{u(x,t)^2}{1 + k \\rho \\int dx' u(x',t)^2}\r r(x,t)=​1+kρ∫dx​′​​u(x​′​​,t)​2​​​​u(x,t)​2​​​​ The intensity of excitatory connection between (x) and (x') J(x,x′)J(x, x')J(x,x​′​​) is given by a Gaussian function: J(x,x′)=12πaexp(−∣x−x′∣22a2)\r J(x,x') = \\frac{1}{\\sqrt{2\\pi}a}\\exp(-\\frac{|x-x'|^2}{2a^2})\r J(x,x​′​​)=​√​2π​​​a​​1​​exp(−​2a​2​​​​∣x−x​′​​∣​2​​​​) The external input IextI_{ext}I​ext​​ is related to position z(t)z(t)z(t): Iext=Aexp[−∣x−z(t)∣24a2]\r I_{ext} = A\\exp\\left[-\\frac{|x-z(t)|^2}{4a^2}\\right]\r I​ext​​=Aexp[−​4a​2​​​​∣x−z(t)∣​2​​​​] While implementing with BrainPy, we create a class of CANN1D by inheriting bp.NeuGroup. Then we define the functions. Where the functions dist and make_conn are designed to get the connection strength JJJ between each of the two neuron groups. In the make_conn function, we first calculate the distance matrix between each of the two xxx. Because neurons are arranged in rings, the value of xxx is between −π-\\pi−π and π\\piπ, so the range of ∣x−x′∣|x-x'|∣x−x​′​​∣ is 2π2\\pi2π, and -π\\piπ and π\\piπ are the same points (the actual maximum value is π\\piπ, that is, half of z_range, the distance exceeded needs to be subtracted from a z_range). We use the dist function to handle the distance on the ring. The get_stimulus_by_pos function processes external inputs based on position pos, which allows users to get input current by setting target positions. For example, in a simple population coding, we give an external input of pos=0, and we run in this way: cann = CANN1D(num=512, k=0.1, monitors=['u']) I1 = cann.get_stimulus_by_pos(0.) Iext, duration = bp.inputs.constant_current([(0., 1.), (I1, 8.), (0., 8.)]) cann.run(duration=duration, inputs=('input', Iext)) Then lets plot an animation by calling the bp.visualize.animate_1D function. # define function def plot_animate(frame_step=5, frame_delay=50): bp.visualize.animate_1D(dynamical_vars=[{'ys': cann.mon.u, 'xs': cann.x, 'legend': 'u'}, {'ys': Iext, 'xs': cann.x, 'legend': 'Iext'}], frame_step=frame_step, frame_delay=frame_delay, show=True) # call the function plot_animate(frame_step=1, frame_delay=100) We can see that the shape of {% math %}u{% endmath %} encodes the shape of external input. Now we add random noise to the external input to see how the shape of {% math %}u{% endmath %} changes. cann = CANN1D(num=512, k=8.1, monitors=['u']) dur1, dur2, dur3 = 10., 30., 0. num1 = int(dur1 / bp.backend.get_dt()) num2 = int(dur2 / bp.backend.get_dt()) num3 = int(dur3 / bp.backend.get_dt()) Iext = np.zeros((num1 + num2 + num3,) + cann.size) Iext[:num1] = cann.get_stimulus_by_pos(0.5) Iext[num1:num1 + num2] = cann.get_stimulus_by_pos(0.) Iext[num1:num1 + num2] += 0.1 * cann.A * np.random.randn(num2, *cann.size) cann.run(duration=dur1 + dur2 + dur3, inputs=('input', Iext)) plot_animate() We can see that the shape of {% math %}u{% endmath %} remains like a bell shape, which indicates that it can perform template matching based on the input. Now let's give a moving input, we vary the position of the input with np.linspace, we will see that the {% math %}u{% endmath %} will follow the input, i.e., smooth tracking. cann = CANN1D(num=512, k=8.1, monitors=['u']) dur1, dur2, dur3 = 20., 20., 20. num1 = int(dur1 / bp.backend.get_dt()) num2 = int(dur2 / bp.backend.get_dt()) num3 = int(dur3 / bp.backend.get_dt()) position = np.zeros(num1 + num2 + num3) position[num1: num1 + num2] = np.linspace(0., 12., num2) position[num1 + num2:] = 12. position = position.reshape((-1, 1)) Iext = cann.get_stimulus_by_pos(position) cann.run(duration=dur1 + dur2 + dur3, inputs=('input', Iext)) plot_animate() Reference 1. Wong, K.-F. & Wang, X.-J. A Recurrent Network Mechanism of Time Integration in Perceptual Decisions. J. Neurosci. 26, 1314–1328 (2006). ↩ 2. Si Wu, Kosuke Hamaguchi, and Shun-ichi Amari. \"Dynamics and computation of continuous attractors.\" Neural computation 20.4 (2008): 994-1025. ↩ "}}