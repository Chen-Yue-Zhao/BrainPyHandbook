{"./":{"url":"./","title":"0. 简介","keywords":"","body":"前言 在本手册中，我们将介绍一系列经典的计算神经科学模型，包括神经元模型、突触模型和网络模型，并提供它们的BrainPy——基于Python的计算神经科学及类脑计算平台——实现。 我们希望，本手册不仅能列出模型的定义、功能，也能对计算神经科学这一学科的脉络和思想有所涉及。通过阅读本手册，读者若能建立对计算神经科学的基本认识，知道如何在学术或应用场景中选择合适的模型、或对现象进行适当的建模，那就是我们在编辑本书时所期望的。 此外，模型后附BrainPy实现代码，帮助初学者快速上手，完成第一次仿真。对于熟悉计算神经科学的读者，我们也希望书中的例子能帮助大家了解BrainPy的优势、学习BrainPy的使用。 BrainPy介绍 在正式开始之前，我们希望先为读者简单介绍如何使用BrainPy实现计算神经科学模型，以方便读者理解附在每个模型之后的BrainPy实现代码。 BrainPy是一个用于计算神经科学和类脑计算的Python平台。要使用BrainPy进行建模，用户通常需要完成以下三个步骤： 1）为神经元和突触模型定义Python类。BrainPy预先定义了数种基类，用户在实现特定模型时，只需继承相应的基类，并在模型的Python类中定义特定的方法来告知BrainPy该模型在仿真的每个时刻所需的操作。在此过程中，BrainPy在微分方程（如ODE、SDE等）的数值积分、多种后端（如Numpy、PyTorch等）适配等功能上辅助用户，简化实现的代码逻辑。 2）将模型的Python类实例化为代表神经元群或突触群的对象，将这些对象传入到BrainPy的Network类的构造函数中，初始化一个网络，并调用run方法进行仿真。 3）调用BrainPy的测度模块measure或可视化模块visualize等，展示仿真结果。 带着上述对BrainPy的粗略理解，我们希望下述各节中的代码实例能够帮助读者更好地理解计算神经科学模型和其中蕴含的思想。下面，我们将按照神经元模型, 突触模型和网络模型的顺序进行介绍。 关于BrainPy的下载及使用上的更多细节，请参考我们的Github仓库：https://github.com/PKU-NIP-Lab/BrainPy。 "},"neurons.html":{"url":"neurons.html","title":"1. 神经元模型","keywords":"","body":"1. 神经元模型 本章首先介绍了建模神经元的基础——生物背景，随后按照从繁到简的顺序，介绍三类主要的神经元模型：生理模型、简化模型和发放率模型。 注：本章所述模型的完整BrainPy代码请见附录，或右键点此下载jupyter notebook版本。 1.1 生物背景 1.2 生理模型 1.3 简化模型 1.4 发放率模型 "},"neurons/biological_background.html":{"url":"neurons/biological_background.html","title":"1.1 生物背景","keywords":"","body":"1.1 生物背景 作为神经系统的基本单位，神经元曾经在很长的一段时间内对研究者保持着神秘。直到18世纪，人们还普遍认为神经通过液体的流动与脑相联系。但到了19世纪，神经生物学取得了长足的进步，当时提出的“神经纤维”这一概念在过去的两个世纪中几经修正，终于演化成为今天我们所说的神经元。 与此同时，随着实验技术的进步，学界已为这些在我们神经系统中无休无止地工作的小东西画出了一张基本的肖像。要想用计算神经科学的方法建模神经元，我们必须先从这张真实细胞膜的肖像入手。 图1-1 神经元细胞膜示意图（Bear et al., 20151） 上图是一张带有离子通道和磷脂双层膜的神经元膜一般性示意图。细胞膜将离子和液体划分为胞内和胞外两侧，部分地限制了胞内和胞外的物质交换，两侧离子不能自由交换达到电中性，于是产生了膜电位，即细胞膜两侧的电位差。 细胞膜内外环境状态的改变会引发膜电位的变化。存在在细胞膜附近（不管是膜内还是膜外）的一个离子主要受两种力的支配：细胞内外离子浓度差产生的扩散力和细胞内外电位差产生的电场力。当这两种力达到平衡时，离子的总受力为零，每种离子都达到其自身的离子平衡电位。与此同时，神经元的膜电位维持在一个小于零的值。 这个由所有离子平衡电位整合而成的膜电位称为静息电位，神经元则在此时进入所谓的静息状态。若不受外部干扰，神经元将自发寻找到平衡的静息状态，并维持在这一状态。 然而，从外部输入到循环输入，从刺激输入到噪声输入，每一毫秒，神经系统都接收到不计其数的外部扰动。面对这些输入，神经元发放动作电位（或峰电位）来在神经系统中处理、传递信息。 图1-2 动作电位（改编自Bear et al., 20151） 图1-2中画出了神经元膜电位在一个动作电位中随时间的变化。由于比如说，外部输入引起的环境变化，图1-1中疏水性磷脂双层膜上的离子通道会在打开和关闭的状态之间切换，调控着离子穿过离子通道进行交换的速率。 在受到外界兴奋性刺激时，特定的离子通道（主要是Na+通道和K+通道）状态发生改变，膜两侧相应离子的浓度变化，引发膜电位的剧变：它先上升到一个峰值，随后在短时间内迅速跌回一个小于静息电位的值。生物上，当膜电位发生这样的一系列变化时，我们说神经元产生了动作电位，或峰电位，或说神经元发放。 一个动作电位基本可以被分为三个阶段，去极化、复极化和不应期。在去极化阶段，钠离子流入细胞，钾离子流出细胞，但钠离子的流入速度更快，因此膜电位从低的静息电位（约-70mV）开始缓慢升高，随后，当膜电位高于阈值电位（约-55mV）后，离子流入和流出速度之间的差值逐渐增大，膜电位快速增长到大于0的峰值（约+40mV）。到达峰值后，钾离子流出速度变得大于钠离子流入速度，膜电位开始降低，并最终复极化到一个可能低于静息电位的值。此后，由于相对更低的膜电位以及离子通道的失活，神经元在短时间内立刻产生另一个动作电位的概率极小，这种情况将一直维持到我们称作不应期的这段时间结束。 单个动作电位的产生已经称得上复杂，但要知道，一个神经元可以在一秒之内产生多个动作电位。这些动作电位是以什么样的模式被产生的？不同类型的神经元可能在面对不同的输入时产生动作电位，而它们发放的特征可以被分为数种发放模式，下图画出了其中一部分。 图1-3 部分神经元发放模式 在单神经元层面上，动作电位的形状和上述的发放模式正是计算神经科学的建模目标。 参考资料 [1] Bear, Mark, Barry Connors, and Michael A. Paradiso. Neuroscience: Exploring the brain, Fourth Edition. Jones & Bartlett Learning, LLC, 2015. "},"neurons/biophysical_models.html":{"url":"neurons/biophysical_models.html","title":"1.2 生理模型","keywords":"","body":"1.2 生理模型 计算神经科学所希望建模的是真实生物的神经系统，因此，要了解神经元模型，可以先从和生理实际联系最紧密的模型入手。下面，本节将介绍受生理实验启发的最经典的模型：Hodgkin-Huxley模型。 1.2.1 Hodgkin-Huxley模型 Hodgkin和Huxley（1952）在枪乌贼的巨轴突上用膜片钳技术记录了动作电位的产生，并提出了经典的神经元模型Hodgkin-Huxley模型（HH模型）。 上一节我们已经介绍了神经元膜的一般结构，为了建模这样的结构，HH模型中将生物上的细胞膜转化为等效电路，如图1-4所示。 图1-4 神经元细胞膜的等效电路图（Gerstner et al., 2014 1） 上图是将图1-1中真实神经元膜转换为电子元件所得到的等效电路图，图中电容CCC表示低电导的疏水性磷脂双层膜，电流III表示外界输入。 右侧三个并联的电阻中，Na+和K+通道被单独建模为两个可变电阻RNaR_{Na}R​Na​​和RKR_KR​K​​（这是由于Na+和K+在动作电位的形成中特别重要），电阻RRR则代表膜上除了Na+通道和K+通道之外所有未指明的离子通道，有时也表示为下标L_L​L​​或者leak_{leak}​leak​​。电源 ENaE_{Na}E​Na​​, EKE_KE​K​​ 和ELE_LE​L​​对应着由相应离子在膜两侧的浓度差所引起的电位差。 考虑基尔霍夫第一定律，即对于电路中的任一点，流入该点的总电流和流出该点的总电流相等，图1-4可被建模为如下所示的微分方程： CdVdt=−(g¯Nam3h(V−ENa)+g¯Kn4(V−EK)+gleak(V−Eleak))+I(t) C \\frac{dV}{dt} = -(\\bar{g}_{Na} m^3 h (V - E_{Na}) + \\bar{g}_K n^4(V - E_K) + g_{leak}(V - E_{leak})) + I(t) C​dt​​dV​​=−(​g​¯​​​Na​​m​3​​h(V−E​Na​​)+​g​¯​​​K​​n​4​​(V−E​K​​)+g​leak​​(V−E​leak​​))+I(t) dxdt=αx(1−x)−βx,x∈{Na,K,leak} \\frac{dx}{dt} = \\alpha_x(1-x) - \\beta_x , x \\in \\{ Na, K, leak \\} ​dt​​dx​​=α​x​​(1−x)−β​x​​,x∈{Na,K,leak} 这就是著名的Hodgkin-Huxley模型。注意在如上的dVdt\\frac{dV}{dt}​dt​​dV​​方程中，右侧的前三项分别代表穿过钠离子通道，钾离子通道和其他非特定离子通道的电流，同时I(t)I(t)I(t)表示外部输入。在方程左侧，CdVdt=dQdt=IC\\frac{dV}{dt} = \\frac{dQ}{dt} = IC​dt​​dV​​=​dt​​dQ​​=I是穿过电容的电流。 在计算通过离子通道的电流时，除了欧姆定律I=U/R=gUI = U/R = gUI=U/R=gU之外，HH模型还引入了三个门控变量m、n和h来表示离子通道的打开/关闭状态。准确地说，变量m和h控制着钠离子通道的状态，变量n控制着钾离子通道的状态。一个离子通道的真实电导是其最大电导g¯\\bar{g}​g​¯​​和通道门控变量状态的乘积，比如通过Na+通道的电流I=U/RNa=gNa∗UNa=g¯∗m3h∗(V−ENa)I=U/R_{Na} = g_{Na}*U_{Na} = \\bar{g} * m^3 h * (V-E_{Na})I=U/R​Na​​=g​Na​​∗U​Na​​=​g​¯​​∗m​3​​h∗(V−E​Na​​)。 门控变量的动力学可以被表示为一种类马尔可夫的形式，其中αx\\alpha_xα​x​​代表门控变量xxx的激活速率，而βx\\beta_xβ​x​​代表xxx的失活速率。下述αx\\alpha_xα​x​​和βx\\beta_xβ​x​​的公式由实验数据拟合得到。 αm(V)=0.1(V+40)1−exp(−(V+40)10) \\alpha_m(V) = \\frac{0.1(V+40)}{1 - exp(\\frac{-(V+40)}{10})} α​m​​(V)=​1−exp(​10​​−(V+40)​​)​​0.1(V+40)​​ βm(V)=4.0exp(−(V+65)18) \\beta_m(V) = 4.0 exp(\\frac{-(V+65)}{18}) β​m​​(V)=4.0exp(​18​​−(V+65)​​) αh(V)=0.07exp(−(V+65)20) \\alpha_h(V) = 0.07 exp(\\frac{-(V+65)}{20}) α​h​​(V)=0.07exp(​20​​−(V+65)​​) βh(V)=11+exp(−(V+35)10) \\beta_h(V) = \\frac{1}{1 + exp(\\frac{-(V + 35)}{10})} β​h​​(V)=​1+exp(​10​​−(V+35)​​)​​1​​ αn(V)=0.01(V+55)1−exp(−(V+55)10) \\alpha_n(V) = \\frac{0.01(V+55)}{1 - exp(\\frac{-(V+55)}{10})} α​n​​(V)=​1−exp(​10​​−(V+55)​​)​​0.01(V+55)​​ βn(V)=0.125exp(−(V+65)80) \\beta_n(V) = 0.125 exp(\\frac{-(V+65)}{80}) β​n​​(V)=0.125exp(​80​​−(V+65)​​) BrainPy仿真的HH模型的V-t图如下所示。我们在上一节中曾经提到，真实的动作电位可以分为去极化、复极化和不应期三个阶段，这三个阶段可以与下图一一对应。另外，在去极化时，可以看到膜电位先是累积外部输入缓慢上升，一旦越过某个特定值（图中约在-55mV左右）就转为快速增长，这也复现了真实动作电位的形状。 参考资料 [1] Gerstner, Wulfram, et al. Neuronal dynamics: From single neurons to networks and models of cognition. Cambridge University Press, 2014. "},"neurons/reduced_models.html":{"url":"neurons/reduced_models.html","title":"1.3 简化模型","keywords":"","body":"1.3 简化模型 启发自生理实验的Hodgkin-Huxley模型准确但昂贵，因此，研究者们提出了简化模型，希望能降低仿真的运行时间和计算资源消耗。 简化模型的特点是简单、易于计算，同时仍可复现神经元发放的主要特征。尽管它们的表示能力常常不如生理模型，但因为其简洁性，研究者们有时也可以接受一定的精度损失。 本节将从简单到复杂，依次介绍：泄露积分-发放模型、二次积分-发放模型、指数积分-发放模型、适应性指数积分-发放模型、Hindmarsh-Rose模型和归纳积分-发放模型。 1.3.1 泄漏积分-发放模型 最经典的简化模型，莫过于Lapicque（1907）提出的泄漏积分-发放模型（Leaky Integrate-and-Fire model, LIF model）。LIF模型是由微分方程表示的积分过程和由条件判断表示的发放过程的结合： τdVdt=−(V−Vrest)+RI(t) \\tau\\frac{dV}{dt} = - (V - V_{rest}) + R I(t) τ​dt​​dV​​=−(V−V​rest​​)+RI(t) If V>VthV > V_{th}V>V​th​​, neuron fires, V←Vreset V \\gets V_{reset} V←V​reset​​ 其中τ=RC\\tau = RCτ=RC是LIF模型的时间常数，τ\\tauτ越大，模型的动力学就越慢。LIF模型同样可以对应到等效电路图上，但比HH模型的等效电路更加简单，因为它不再建模钠离子通道和钾离子通道。实际上，LIF模型中只有电阻RRR，电容CCC，电源VrestV_{rest}V​rest​​和外部输入III被建模。 图1-5 LIF模型对应的细胞膜等效电路图（简化后） 尽管LIF模型可以产生动作电位，但没有建模动作电位的形状。在发放动作电位前，LIF神经元膜电位的增长速度将逐渐降低，而并非像真实神经元那样先缓慢增长，在跨过阈值电位之后转为迅速增长。 原始LIF模型还忽略了不应期。要模拟不应期，必须再补充一个条件判断：如果当前时刻距离上次发放的时间小于不应期时长，则神经元处于不应期，膜电位VVV不再更新。 1.3.2 二次积分-发放模型 LIF模型固然简洁，但像上一节末尾所讲的那样，有着诸多限制。为了弥补它在表示能力上的缺陷，Latham等人（2000）提出了二次积分-发放模型（Quadratic Integrate-and-Fire model，QuaIF model）。在QuaIF模型中，微分方程右侧的二阶项使得神经元能产生和真实神经元更“像”的动作电位。 τdVdt=a0(V−Vrest)(V−Vc)+RI(t) \\tau\\frac{d V}{d t}=a_0(V-V_{rest})(V-V_c) + RI(t) τ​dt​​dV​​=a​0​​(V−V​rest​​)(V−V​c​​)+RI(t) 在上式中，a0a_0a​0​​和VCV_CV​C​​共同控制着动作电位的初始化，其中，a0a_0a​0​​控制着发放前膜电位的增长速度，也即膜电位相对时间变化的斜率；VcV_cV​c​​是动作电位初始化的临界值，当膜电位VVV低于 VCV_CV​C​​时，VVV缓慢增长，一旦越过 VCV_CV​C​​， VVV就转为迅速增长。 1.3.3 指数积分-发放模型 指数积分-发放模型（Exponential Integrate-and-Fire model, ExpIF model）（Fourcaud-Trocme et al., 2003）在QuaIF模型的基础上更上一层，进一步提升了模型生成的动作电位的真实度。 τdVdt=−(V−Vrest)+ΔTeV−VTΔT+RI(t) \\tau \\frac{dV}{dt} = - (V - V_{rest}) + \\Delta_T e^{\\frac{V - V_T}{\\Delta_T}} + R I(t) τ​dt​​dV​​=−(V−V​rest​​)+Δ​T​​e​​Δ​T​​​​V−V​T​​​​​​+RI(t) 在指数项中VTV_TV​T​​是动作电位初始化的临界值，在其下VVV缓慢增长，其上VVV迅速增长。ΔT\\Delta_TΔ​T​​是ExpIF模型中动作电位的斜率。当ΔT→0\\Delta_T\\to 0Δ​T​​→0时，ExpIF模型中动作电位的形状将趋近于Vth=VTV_{th} = V_TV​th​​=V​T​​的LIF模型（Fourcaud-Trocme et al.，2003）。 在上图中可以看到，ExpIF模型中膜电位相对于时间的斜率在每次上升到VTV_TV​T​​值（-59.9mV）附近时，都发生了一个明显的转变，这是由于微分方程中指数项的调控。比起QuaIF模型，这种转变显得更加自然。 1.3.4 适应性指数积分-发放模型 在以上诸积分-发放模型中，建模了神经元的标准动作电位，但尚有许多神经元的行为未被涉及。 让我们稍稍离题，请读者做这样一种想象：你独自一人来到夜晚的海边，一开始，你闻到海风里的腥味，忍不住深吸一口气（或者捂住鼻子）。但过了一会儿，不管主观上想要亲近还是远离大海，你再也闻不见这种腥气——或者至少是以为自己闻不见了。这是因为你的嗅觉系统习惯了这种刺激，不再无休无止地提醒你的大脑附近存在着异味了。 上面这个例子中，对鱼腥味发生适应的是整个嗅觉感知系统。不过，在单神经元尺度上，也存在类似的行为。当特定类型的神经元面对恒定的外部刺激时，一开始神经元高频发放，随后发放率逐渐降低，最终稳定在一个较小值，这就是神经元的适应行为。 为了复现这种行为，研究者们在已有的积分-发放模型（如LIF、QuaIF和ExpIF模型等）上增加了权重变量www。这里我们介绍其中的适应性指数积分-发放模型（Adaptive Exponential Integrate-and-Fire model，AdExIF model）（Gerstner et al.，2014）。 τmdVdt=−(V−Vrest)+ΔTeV−VTΔT−Rw+RI(t) \\tau_m \\frac{dV}{dt} = - (V - V_{rest}) + \\Delta_T e^{\\frac{V - V_T}{\\Delta_T}} - R w + R I(t) τ​m​​​dt​​dV​​=−(V−V​rest​​)+Δ​T​​e​​Δ​T​​​​V−V​T​​​​​​−Rw+RI(t) τwdwdt=a(V−Vrest)−w+bτw∑δ(t−tf)) \\tau_w \\frac{dw}{dt} = a(V - V_{rest})- w + b \\tau_w \\sum \\delta(t - t^f)) τ​w​​​dt​​dw​​=a(V−V​rest​​)−w+bτ​w​​∑δ(t−t​f​​)) 如其名所示，AdExIF模型的第一个微分方程和我们上面介绍的ExpIF模型非常相似，唯一区别是方程右侧增加了控制神经元适应行为的权重项，即−Rw-Rw−Rw一项。 权重项中www受到第二个微分方程的调控。aaa描述了权重变量www对VVV的下阈值波动的敏感性，bbb表示www在一次发放后的增长值，另外，www也随时间衰减。 在这样的一个动力学系统中，给神经元一个恒定输入，在连续数次发放后，www的值将会上升到一个高点，减慢VVV的增长速度，从而降低神经元的发放率。 1.3.5 Hindmarsh-Rose模型 神经元的行为并不总是符合标准模板的。比如，不是所有神经元都在每次发放后等待一整个不应期才进行第二次发放。有时，部分神经元面对特定类型的输入，能够产生短时间内的连续发放。和以上所有模型产生的脉冲式发放相对地，我们称这种发放模式为爆发（brusting），或爆发式发放（bursting firing）。 为了模拟神经元的爆发，Hindmarsh和Rose（1984）提出了Hindmarsh-Rose模型，引入了第三个模型变量zzz作为慢变量控制爆发。 dVdt=y−aV3+bV2−z+I \\frac{d V}{d t} = y - a V^3 + b V^2 - z + I ​dt​​dV​​=y−aV​3​​+bV​2​​−z+I dydt=c−dV2−y \\frac{d y}{d t} = c - d V^2 - y ​dt​​dy​​=c−dV​2​​−y dzdt=r(s(V−Vrest)−z) \\frac{d z}{d t} = r (s (V - V_{rest}) - z) ​dt​​dz​​=r(s(V−V​rest​​)−z) 式中，变量VVV表示膜电位，yyy和zzz是两个门控变量。在dV/dtdV/dtdV/dt方程中的参数bbb允许模型在脉冲式发放和爆发式发放之间切换，并控制脉冲式发放的频率。dz/dtdz/dtdz/dt方程中，参数rrr控制慢变量zzz的变化速率，影响神经元爆发式发放时，每次爆发包含的动作电位个数，并和参数bbb共同控制脉冲式发放的频率；参数sss控制着神经元的适应行为。其它参数根据发放模式拟合得到。 下图中画出了VVV、yyy、zzz三个变量随时间的变化。可以看到，慢变量zzz的变化速率确实慢于VVV和yyy。而且，VVV和yyy在仿真过程中呈近似周期性的变化。 图1-6 Hindmarsh-Rose模型变量随时间的变化 利用BrainPy的理论分析模块analysis，我们可以找出这种周期性的产生原因。将慢变量zzz近似为常数，则Hindmarsh-Rose模型的二维相图中，变量VVV和yyy的轨迹趋近于一个极限环。因此，这两个变量的值会沿极限环周期性变化。 1.3.6 归纳积分-发放模型 归纳积分-发放模型（Generalized Integrate-and-Fire model，GeneralizedIF model）（Mihalaş et al.，2009）整合了多种发放模式。该模型包括四个变量，能产生二十种发放模式，并通过调参在各模式之间切换。 τdVdt=−(V−Vrest)+R∑jIj+RI \\tau \\frac{d V}{d t} = - (V - V_{rest}) + R\\sum_{j}I_j + RI τ​dt​​dV​​=−(V−V​rest​​)+R​j​∑​​I​j​​+RI dVthdt=a(V−Vrest)−b(Vth−Vth∞) \\frac{d V_{th}}{d t} = a(V - V_{rest}) - b(V_{th} - V_{th\\infty}) ​dt​​dV​th​​​​=a(V−V​rest​​)−b(V​th​​−V​th∞​​) dIjdt=−kjIj,j=1,2 \\frac{d I_j}{d t} = - k_j I_j, j = {1, 2} ​dt​​dI​j​​​​=−k​j​​I​j​​,j=1,2 当VVV达到VthV_{th}V​th​​时，神经元发放： Ij←RjIj+Aj I_j \\leftarrow R_j I_j + A_j I​j​​←R​j​​I​j​​+A​j​​ V←Vreset V \\leftarrow V_{reset} V←V​reset​​ Vth←max(Vthreset,Vth) V_{th} \\leftarrow max(V_{th_{reset}}, V_{th}) V​th​​←max(V​th​reset​​​​,V​th​​) 在dV/dtdV/dtdV/dt的方程中，和所有积分-发放模型一样，τ\\tauτ表示时间常数，VVV表示膜电位，VrestV_{rest}V​rest​​表示静息电位，RRR为电阻，而III为外部输入。 不过，在GIF模型中，数目可变的内部电流被加入到方程中，写作∑jIj\\sum_j I_j∑​j​​I​j​​一项。每一个IjI_jI​j​​都代表神经元中的一个内部电流，并以速率kjk_jk​j​​衰减。RjR_jR​j​​和AjA_jA​j​​是自由参数，RjR_jR​j​​描述IjI_jI​j​​重置值对发放前IjI_jI​j​​值的依赖，AjA_jA​j​​则是发放后加到IjI_jI​j​​上的一个常数。 阈值电位VthV_{th}V​th​​也是可变的，受两个参数的调控：aaa 描述了VthV_{th}V​th​​对膜电位VVV 的依赖，bbb描述了VthV_{th}V​th​​接近阈值电位在时间趋近于无穷大时的值Vth∞V_{th_{\\infty}}V​th​∞​​​​的速率。VthresetV_{th_{reset}}V​th​reset​​​​是当神经元发放时，阈值电位被重置到的值。 "},"neurons/firing_rate_models.html":{"url":"neurons/firing_rate_models.html","title":"1.4 发放率模型","keywords":"","body":"1.4 发放率模型 发放率模型比简化模型更加简单。在这些模型中，每个计算单元代表一个神经元群，而单神经元模型中的膜电位变量VVV也被发放率变量aaa（或rrr或ν\\nuν）所取代。本节将介绍一个经典的发放率单元。 1.4.1 发放率单元 Wilson和Cowan（1972）提出这一发放率模型来表示在兴奋性和抑制性皮层神经元微柱中的活动。变量aea_ea​e​​和aia_ia​i​​中的每个元素都表示一个包含复数神经元的皮层微柱中神经元群的平均活动水平。 τedae(t)dt=−ae(t)+(ke−re∗ae(t))∗S(c1ae(t)−c2ai(t)+Iexte(t)) \\tau_e \\frac{d a_e(t)}{d t} = - a_e(t) + (k_e - r_e * a_e(t)) * \\mathcal{S}(c_1 a_e(t) - c_2 a_i(t) + I_{ext_e}(t)) τ​e​​​dt​​da​e​​(t)​​=−a​e​​(t)+(k​e​​−r​e​​∗a​e​​(t))∗S(c​1​​a​e​​(t)−c​2​​a​i​​(t)+I​ext​e​​​​(t)) τidai(t)dt=−ai(t)+(ki−ri∗ai(t))∗S(c3ae(t)−c4ai(t)+Iexti(t)) \\tau_i \\frac{d a_i(t)}{d t} = - a_i(t) + (k_i - r_i * a_i(t)) * \\mathcal{S}(c_3 a_e(t) - c_4 a_i(t) + I_{ext_i}(t)) τ​i​​​dt​​da​i​​(t)​​=−a​i​​(t)+(k​i​​−r​i​​∗a​i​​(t))∗S(c​3​​a​e​​(t)−c​4​​a​i​​(t)+I​ext​i​​​​(t)) S(input)=11+exp(−a(input−θ))−11+exp(aθ) \\mathcal{S}(input) = \\frac{1}{1 + exp(- a(input - \\theta))} - \\frac{1}{1 + exp(a\\theta)} S(input)=​1+exp(−a(input−θ))​​1​​−​1+exp(aθ)​​1​​ 下标x∈{e,i}x\\in\\{e, i\\}x∈{e,i}表示该参数或变量对应兴奋性或抑制性的神经元群。在微分方程中，τx\\tau_xτ​x​​表示神经元群的时间常数，参数kxk_xk​x​​和rxr_xr​x​​共同控制不应期，axa_xa​x​​和θx\\theta_xθ​x​​分别是Sigmoid函数S(input)\\mathcal{S}(input)S(input)的斜率和相位，且兴奋性和抑制性的神经元群分别收到外界输入IextxI_{ext_{x}}I​ext​x​​​​。 "},"synapses.html":{"url":"synapses.html","title":"2. 突触模型","keywords":"","body":"2. 突触模型 当我们建模了神经元的动作电位后，我们需要建立突触模型来描述神经元之间的信息传递过程。突触把神经元连接起来，使不同神经元得以沟通，对于组成神经网络也是至关重要的。 本章将在2.1 突触模型中介绍包括化学突触与电突触的模型，其中化学突触包括常见的AMPA模型、NMDA模型等，以及更加抽象的、简化的Alpha模型、单指数衰减模型等。 突触模型另一个重要的方面在于突触可塑性的实现，突触可塑性对学习、记忆与神经网络的计算、训练非常重要，本章将会在2.2 可塑性模型中介绍突触可塑性的部分，包括突触短时程可塑性（STP）及突触长时程可塑性等。 注：本章所述模型的完整BrainPy代码请见附录，或右键点此下载jupyter notebook版本。 2.1 突触模型 2.2 可塑性模型 "},"synapses/dynamics.html":{"url":"synapses/dynamics.html","title":"2.1 突触动力学模型","keywords":"","body":"2.1 突触模型 我们在前面的章节中已经学习了如何建模神经元的动作电位，那么神经元之间是怎么连接起来的呢？神经元的动作电位是如何在不同神经元之间传导的呢？这里，我们将介绍如何用BrainPy来模拟神经元之间的沟通。 注：本章所述模型的完整BrainPy代码请见附录，或右键点此下载jupyter notebook版本。 2.1.1 化学突触 生物背景 图2-1描述了神经元之间信息传递的生物过程。当突触前神经元的动作电位传递到轴突的末端（terminal）时，突触前神经元会释放神经递质（又称递质）。神经递质会和突触后神经元上的受体结合，从而引起突触后神经元膜电位的改变，这种改变称为突触后电位（PSP）。根据神经递质种类的不同，突触后电位可以是兴奋性的或抑制性的。例如谷氨酸（Glutamate）就是一种重要的兴奋性神经递质，而GABA则是一种重要的抑制性神经递质。 神经递质与受体的结合可能会导致离子通道的打开（离子型受体）或改变化学反应的过程（代谢型受体）。 在本节中，我们将介绍如何使用BrainPy来实现一些常见的突触模型，主要有： AMPA和NMDA：它们都是谷氨酸的离子型受体，被结合后都可以直接打开离子通道。但是NMDA通常会被镁离子（Mg2+^{2+}​2+​​）堵住，无法对谷氨酸做出反应。由于镁离子对电压敏感，当突触后电位超过镁离子的阈值以后，镁离子就会离开NMDA通道，让NMDA可以对谷氨酸做出反应。因此，NMDA的反应是比较慢的。 GABAA和GABAB：它们是GABA的两类受体，其中GABAA是离子型受体，通常可以产生快速的抑制性电位；而GABAB则为代谢型受体，通常会产生缓慢的抑制性电位。 图 2-1 生物突触 (引自 Gerstner et al., 2014 1) 为了简便地建模从神经递质释放到突触后神经元膜电位改变的过程，我们可以使用门控变量sss来描述每当突触前神经元产生动作电位时，有多少比例的离子通道会被打开。让我们从AMPA的例子开始，看看如何建立突触模型并用BrainPy实现。 AMPA模型 如前所述，AMPA（a-氨基-3-羟基-5-甲基-4-异恶唑丙酸）受体是一种离子型受体，也就是说，当它被神经递质结合后会立即打开离子通道，从而引起突触后神经元膜电位的变化。 我们可以用马尔可夫过程来描述离子通道的开关。如图2-2所示，sss代表通道打开的概率，1−s1-s1−s代表离子通道关闭的概率，α\\alphaα和β\\betaβ是转移概率（transition probability）。由于神经递质能让离子通道打开，所以从1−s1-s1−s到sss的转移概率受神经递质浓度（以[T]表示）影响。 图2-2 离子通道动力学的马尔可夫过程 把该过程用微分方程描述，得到以下式子。 dsdt=α[T](1−s)−βs \\frac {ds}{dt} = \\alpha [T] (1-s) - \\beta s ​dt​​ds​​=α[T](1−s)−βs 其中，α[T]\\alpha [T]α[T] 表示从状态(1−s)(1-s)(1−s)到状态(s)(s)(s)的转移概率，即激活速率；β\\betaβ 表示从sss到(1−s)(1-s)(1−s)的转移概率，即失活速率。 下面我们来看看如何用BrainPy去实现这样一个模型。首先，我们要定义一个类，因为突触是连接两个神经元的，所以这个类继承自bp.TwoEndConn。在这个类中，和神经元模型一样，我们用一个derivative函数来实现上述微分方程，并在后面的__init__函数中初始化这个函数，指定用bp.odeint来解这个方程，并指定数值积分方法。由于这微分方程是线性的，我们选用exponential_euler方法。 然后我们在update函数中更新sss。 我们已经定义好了一个AMPA类，现在可以画出sss随时间变化的图了。我们首先写一个run_syn函数来方便之后运行更多的突触模型，然后把AMPA类和需要自定义的变量传入这个函数来运行并画图。 运行以上代码，即可看到以下的结果： 由上图可以看出，当突触前神经元产生一个动作电位，sss的值会先增加，然后衰减。 NMDA模型 如前所述，NMDA受体一开始被镁离子堵住，而当膜电位达到一定阈值后，镁离子则会移开。我们用cMgc_{Mg}c​Mg​​表示镁离子的浓度，它对突触后膜的电导ggg的影响可以由以下公式描述： g∞=(1+e−αV⋅cMgβ)−1 g_{\\infty} =(1+{e}^{-\\alpha V} \\cdot \\frac{c_{Mg} } {\\beta})^{-1} g​∞​​=(1+e​−αV​​⋅​β​​c​Mg​​​​)​−1​​ g=g¯⋅g∞s g = \\bar{g} \\cdot g_{\\infty} s g=​g​¯​​⋅g​∞​​s 其中g∞g_{\\infty}g​∞​​代表了镁离子浓度的作用，其值随着镁离子浓度增加而减小；而随着电压VVV增加，g∞g_{\\infty}g​∞​​较不受cMgc_{Mg}c​Mg​​影响。α,β\\alpha, \\betaα,β和g¯\\bar{g}​g​¯​​是一些常数。门控变量sss和AMPA模型类似，其动力学由以下公式给出： dsdt=−sτdecay+ax(1−s) \\frac{d s}{dt} =-\\frac{s}{\\tau_{\\text{decay}}}+a x(1-s) ​dt​​ds​​=−​τ​decay​​​​s​​+ax(1−s) dxdt=−xτrise \\frac{d x}{dt} =-\\frac{x}{\\tau_{\\text{rise}}} ​dt​​dx​​=−​τ​rise​​​​x​​ if (pre fire), then x←x+1 \\text{if (pre fire), then} \\ x \\leftarrow x+ 1 if (pre fire), then x←x+1 其中，τdecay\\tau_{\\text{decay}}τ​decay​​和τrise\\tau_{\\text{rise}}τ​rise​​分别为sss衰减及上升的时间常数，aaa是参数。 接下来我们用BrainPy来实现NMDA模型，代码如下。 由于我们在实现AMPA模型时已经定义了run_syn函数，在这里我们可以直接调用： run_syn(NMDA) 由图可以看出，NMDA的衰减过程非常缓慢，第一个突触前神经元的动作电位引起的sss增加后还没怎么衰减，第二个的值就加上去了，由于我们这里只跑了30ms的模拟，还看不到NMDA衰退的过程。 GABAB模型 GABAB是一种代谢型受体，神经递质和受体结合后不会直接打开离子通道，而是通过G蛋白作为第二信使来起作用。因此，这里我们用[R][R][R]表示多少比例的受体被激活，并用[G][G][G]表示激活的G蛋白的浓度，sss由[G][G][G]调节，公式如下： d[R]dt=k3[T](1−[R])−k4[R] \\frac{d[R]}{dt} = k_3 [T](1-[R])- k_4 [R] ​dt​​d[R]​​=k​3​​[T](1−[R])−k​4​​[R] d[G]dt=k1[R]−k2[G] \\frac{d[G]}{dt} = k_1 [R]- k_2 [G] ​dt​​d[G]​​=k​1​​[R]−k​2​​[G] s=[G]4[G]4+Kd s =\\frac{[G]^{4}} {[G]^{4}+K_{d}} s=​[G]​4​​+K​d​​​​[G]​4​​​​ [R][R][R]的动力学类似于AMPA模型中的sss，受神经递质浓度[T][T][T]影响，k3,k4k_3, k_4k​3​​,k​4​​表示转移概率。[G][G][G]的动力学受[R][R][R]影响，并由参数k1,k2k_1, k_2k​1​​,k​2​​控制。KdK_dK​d​​为一个常数。 用BrainPy实现的代码如下。 由于GABAB的动力学和NMDA一样，也是非常缓慢的，这里我们不再用前面写的只有30ms模拟的run_syn函数，而是通过调用BrainPy提供的bp.inputs.constant_current方法，先给20ms的输入，接着看剩余1000ms在没有外界输入情况下的衰减。 neu1 = bm.neurons.LIF(2, monitors=['V']) neu2 = bm.neurons.LIF(3, monitors=['V']) syn = GABAb(pre=neu1, post=neu2, conn=bp.connect.All2All(), monitors=['s']) net = bp.Network(neu1, syn, neu2) # input I, dur = bp.inputs.constant_current([(25, 20), (0, 1000)]) net.run(dur, inputs=(neu1, 'input', I)) bp.visualize.line_plot(net.ts, syn.mon.s, ylabel='s', show=True) 结果显示，GABAB的衰减持续数百毫秒。 基于电流与基于电导的模型 细心的读者应该发现，我们刚才对GABAB门控变量sss的建模并没有显示出其引起抑制性电位的特点。要体现出兴奋性和抑制性，我们不仅需要建模门控变量sss，还需要建模通过突触的电流III（作为突触后神经元的输入）。根据突触电流是否受突触后神经元膜电位的影响不同，分为基于电流（current-based）与基于电导（conductance-based）的两种模型。 （1）基于电流（Current-based）的模型 基于电流的模型公式如下： I∝s I \\propto s I∝s 在代码实现上，我们通常会乘上一个权重www。我们可以通过调整权重www的正负值来实现兴奋性和抑制性突触。另外，我们通过使用BrainPy提供的register_constant_delay函数给变量I_syn加上延迟时间来实现突触的延迟。 （2）基于电导（Conductance-based）的模型 在基于电导的模型中，电导为g=g¯sg=\\bar{g}sg=​g​¯​​s。因此，根据欧姆定律得公式如下： I=g¯s(V−E) I=\\bar{g}s(V-E) I=​g​¯​​s(V−E) 这里EEE是一个反转电位（reverse potential），它可以决定III的方向是抑制还是兴奋。例如，当静息电位约为-65mV时，减去比它更低的EEE，例如-75mV，将变为正，从而改变公式中电流的方向并产生抑制电流。兴奋性突触的EEE一般为比较高的值，如0mV。 代码实现上，可以把延迟时间应用到变量g上。 现在可以回顾一下我们刚才实现的NMDA模型和GABAB模型，它们都是基于电导的模型，在NMDA模型中，E=0mVE=0mVE=0mV，因此产生兴奋性电流；而在GABAB模型中，E=−95mVE=-95mVE=−95mV，产生抑制性电流。 抽象的简化模型 前面我们建模了几种经典的化学突触模型，它们的门控变量sss的动力学都有着先上升后下降的特征。当我们不需要具体地建模某种生物学突触时，只要把握了突触的基本动力学特征（先上升后下降）即可。这里，我们会介绍四种抽象的简化模型及其在BrainPy上的实现，这些抽象模型既可以是基于电流的，也可以是基于电导的模型，可以根据需要选择。 (1) 双指数差（Differences of two exponentials） 我们首先来看双指数差（Differences of two exponentials）模型，它有两个指数项相减，公式如下： s=τ1τ2τ1−τ2(exp(−t−tsτ1)−exp(−t−tsτ2)) s = \\frac {\\tau_1 \\tau_2}{\\tau_1 - \\tau_2} (\\exp(-\\frac{t - t_s}{\\tau_1}) - \\exp(-\\frac{t - t_s}{\\tau_2})) s=​τ​1​​−τ​2​​​​τ​1​​τ​2​​​​(exp(−​τ​1​​​​t−t​s​​​​)−exp(−​τ​2​​​​t−t​s​​​​)) 其中 tst_st​s​​ 表示突触前神经元产生动作电位的时间，τ1\\tau_1τ​1​​和τ2\\tau_2τ​2​​为时间常数。 在BrainPy的实现中，我们采用以下微分方程形式： dsdt=x \t\t\\frac {ds} {dt} = x ​dt​​ds​​=x dxdt=−τ1+τ2τ1τ2x−sτ1τ2 \\frac {dx}{dt} =- \\frac{\\tau_1+\\tau_2}{\\tau_1 \\tau_2}x - \\frac s {\\tau_1 \\tau_2} ​dt​​dx​​=−​τ​1​​τ​2​​​​τ​1​​+τ​2​​​​x−​τ​1​​τ​2​​​​s​​ if (fire), then x←x+1 \\text{if (fire), then} \\ x \\leftarrow x+ 1 if (fire), then x←x+1 这里我们用update函数来控制xxx增加的逻辑。代码如下： (2) Alpha突触 Alpha突触的动力学由以下公式给出： s=t−tsτexp(−t−tsτ) s = \\frac{t - t_s}{\\tau} \\exp(-\\frac{t - t_s}{\\tau}) s=​τ​​t−t​s​​​​exp(−​τ​​t−t​s​​​​) 和双指数差模型类似， tst_st​s​​ 表示突触前神经元产生动作电位的时间，不同的是这里只有一个时间常数τ\\tauτ。微分方程形式如下： dsdt=x \\frac {ds} {dt} = x ​dt​​ds​​=x dxdt=−2xτ−sτ2 \\frac {dx}{dt} =- \\frac{2x}{\\tau} - \\frac s {\\tau^2} ​dt​​dx​​=−​τ​​2x​​−​τ​2​​​​s​​ if (fire), then x←x+1 \\text{if (fire), then} \\ x \\leftarrow x+ 1 if (fire), then x←x+1 可以看出alpha模型和双指数差模型其实很相似，相当于是τ=τ1=τ2\\tau=\\tau_1 = \\tau_2τ=τ​1​​=τ​2​​。因此，代码实现上也很接近： (3) 单指数衰减（Single exponential decay） 下面我们来介绍一种更加简化的模型，它忽略了上升的过程，而只建模了衰减（decay）的过程。单指数衰减（Single exponential decay）模型用一个指数项来描述衰减的过程，公式如下： dsdt=−sτdecay \\frac {ds}{dt}=-\\frac s {\\tau_{decay}} ​dt​​ds​​=−​τ​decay​​​​s​​ if (fire), then s←s+1 \\text{if (fire), then} \\ s \\leftarrow s+1 if (fire), then s←s+1 代码实现如下： (4) 电压跳变（Voltage jump） 电压跳变（Voltage jump）模型比单指数衰减模型还要更加简化，它连衰退的过程也忽略了，公式如下： if (fire), then s←s+1 \\text{if (fire), then} \\ s \\leftarrow s+1 if (fire), then s←s+1 在实现上，只需要在update函数中更新sss即可。代码如下： 2.1.2 电突触 除了前面介绍的化学突触以外，电突触在我们神经系统中也很常见。 (a) (b) 图2-3 (a) 神经元间的缝隙连接. (b) 等效模型. (引自 Sterratt et al., 2011 2) 如图2-3a所示，两个神经元通过连接通道（junction channels）相连，可以直接导电，这种连接又称为缝隙连接（gap junction）。因此，可以看作是两个神经元由一个常数电阻连起来，如图2-3b所示。 根据欧姆定律可得以下公式： I1=w(V0−V1) I_{1} = w (V_{0} - V_{1}) I​1​​=w(V​0​​−V​1​​) 这里V0V_0V​0​​和V1V_1V​1​​分别为两个神经元的膜电位，突触权重www表示常数电导。 在BrainPy的实现中，只需要在update函数里更新即可。 定义好了缝隙连接的类以后，我们跑模拟来看给0号神经元输入时，1号神经元的电位变化。我们首先实例化两个LIF神经元模型，并用缝隙连接把它们连接起来。然后仅给0号神经元neu0一个恒定的电流，neu1没有外界输入。 import matplotlib.pyplot as plt neu0 = bm.neurons.LIF(1, monitors=['V'], t_refractory=0) neu0.V = bp.ops.ones(neu0.V.shape) * -10. neu1 = bm.neurons.LIF(1, monitors=['V'], t_refractory=0) neu1.V = bp.ops.ones(neu1.V.shape) * -10. syn = Gap_junction(pre=neu0, post=neu1, conn=bp.connect.All2All(), k_spikelet=5.) syn.w = bp.ops.ones(syn.w.shape) * .5 net = bp.Network(neu0, neu1, syn) net.run(100., inputs=(neu0, 'input', 30.)) fig, gs = bp.visualize.get_figure(row_num=2, col_num=1, ) fig.add_subplot(gs[1, 0]) plt.plot(net.ts, neu0.mon.V[:, 0], label='V0') plt.legend() fig.add_subplot(gs[0, 0]) plt.plot(net.ts, neu1.mon.V[:, 0], label='V1') plt.legend() plt.show() 结果图中，下图V0V_0V​0​​表示0号神经元的膜电位变化，而上图V1V_1V​1​​为1号神经元的膜电位。0号神经元因为有电流输入而有持续的发放，并给1号神经元输入，导致V1V1V1产生阈值下的改变。 参考资料 [1] Gerstner, Wulfram, et al. Neuronal dynamics: From single neurons to networks and models of cognition. Cambridge University Press, 2014. [2] Sterratt, David, et al. Principles of computational modeling in neuroscience. Cambridge University Press, 2011. "},"synapses/plasticity.html":{"url":"synapses/plasticity.html","title":"2.2 突触可塑性模型","keywords":"","body":"2.2 突触可塑性 在前一节中，我们讨论了突触动力学，但还没有涉及到突触可塑性。接下来我们将在本节中介绍如何使用BrainPy来实现突触可塑性。 突触可塑性指的是突触强度（synaptic efficacy）或突触权重（synaptic weight）的变化，主要分为短时程可塑性（short-term plasticity）与长时程可塑性（long-term plasticity）。我们将首先介绍突触短时程可塑性，然后介绍几种不同的突触长时程可塑性模型。 注：本章所述模型的完整BrainPy代码请见附录，或右键点此下载jupyter notebook版本。 2.2.1 突触短时程可塑性（STP） 我们首先从实验结果来介绍突触短时程可塑性。在图2-4中，上图表示突触前神经元的动作电位，下图为突触后神经元的膜电位。我们可以看到，当突触前神经元在短时间内持续发放的时候，突触后神经元的反应越来越弱，呈现出短时程抑制 (short term depression)效果。而当突触前神经元停止发放几百毫秒后，再来一个动作电位，此时突触后神经元的反应基本恢复到一开始的状态，因此这个抑制效果持续的时间很短，称为短时程可塑性。 图2-4 突触短时程可塑性 (改编自 Gerstner et al., 2014 1) 那么接下来就让我们来看看描述短时程可塑性的计算模型。短时程可塑性主要由神经递质释放的概率uuu和神经递质的剩余量xxx两个变量来描述。整体的动力学方程如下： dIdt=−Iτ \\frac {dI} {dt} = - \\frac I {\\tau} ​dt​​dI​​=−​τ​​I​​ dudt=−uτf \\frac {du} {dt} = - \\frac u {\\tau_f} ​dt​​du​​=−​τ​f​​​​u​​ dxdt=1−xτd \\frac {dx} {dt} = \\frac {1-x} {\\tau_d} ​dt​​dx​​=​τ​d​​​​1−x​​ if (pre fire), then{u+=u−+U(1−u−)I+=I−+Au+x−x+=x−−u+x− \\text{if (pre fire), then} \\begin{cases} u^+ = u^- + U(1-u^-) \\\\ I^+ = I^- + Au^+x^- \\\\ x^+ = x^- - u^+x^- \\end{cases} if (pre fire), then​⎩​⎪​⎨​⎪​⎧​​​u​+​​=u​−​​+U(1−u​−​​)​I​+​​=I​−​​+Au​+​​x​−​​​x​+​​=x​−​​−u​+​​x​−​​​​ 其中，突触电流III的动力学可以采用上一节介绍的任意一种sss的动力学模型，这里我们采用简单、常用的单指数衰减（single exponential decay）模型来描述。UUU和AAA分别为uuu和III的增量，而τf\\tau_fτ​f​​和τd\\tau_dτ​d​​则分别为uuu和xxx的时间常数。 在该模型中，uuu主要贡献了短时程易化（Short-term facilitation；STF）的效果，它的初始值为0，并随着突触前神经元的每次发放而增加；而xxx则主要贡献短时程抑制（Short-term depression；STD）效果，它的初始值为1，并在每次突触前神经元发放时都会被用掉一些（即减少）。易化和抑制两个方向的效果是同时发生的，因此τf\\tau_fτ​f​​和τd\\tau_dτ​d​​的大小关系决定了可塑性的哪个方向的变化起主要作用。 用BrainPy实现的代码如下，由于突触可塑性也是发生在突触上的，这里和突触模型一样，继承自bp.TwoEndConn。 定义好STP的类以后，接下来让我们来定义跑模拟的函数。跟突触模型一样，我们需要实例化两个神经元群并把它们连接在一起。结果画图方面，除了sss的动力学以外，我们也希望看到uuu和xxx随时间的变化，因此我们制定monitors=['s', 'u', 'x']。 def run_stp(**kwargs): neu1 = bm.neurons.LIF(1, monitors=['V']) neu2 = bm.neurons.LIF(1, monitors=['V']) syn = STP(pre=neu1, post=neu2, conn=bp.connect.All2All(), monitors=['s', 'u', 'x'], **kwargs) net = bp.Network(neu1, syn, neu2) net.run(100., inputs=(neu1, 'input', 28.)) # plot fig, gs = bp.visualize.get_figure(2, 1, 3, 7) fig.add_subplot(gs[0, 0]) plt.plot(net.ts, syn.mon.u[:, 0], label='u') plt.plot(net.ts, syn.mon.x[:, 0], label='x') plt.legend() fig.add_subplot(gs[1, 0]) plt.plot(net.ts, syn.mon.s[:, 0], label='s') plt.legend() plt.xlabel('Time (ms)') plt.show() 接下来，我们设 tau_d > tau_f，让我们来看看结果。 run_stp(U=0.2, tau_d=150., tau_f=2.) 从结果图中，我们可以看出当设置 τd>τf\\tau_d > \\tau_fτ​d​​>τ​f​​时，xxx每次用掉以后恢复得很慢，而uuu每次增加后很快又衰减下去了，因此从sss随时间变化的图中我们可以看到STD效果为主。 接下来看看当我们设置tau_f > tau_d时的结果。 run_stp(U=0.1, tau_d=10, tau_f=100.) 结果图显示，当τf>τd\\tau_f > \\tau_dτ​f​​>τ​d​​时，xxx每次用掉后很快又补充回去了，这表示突触前神经元总是有足够的神经递质可用。同时，uuu的衰减非常缓慢，即释放神经递质的概率越来越高，从sss的动力学可以看出STF效果占主要地位。 2.2.2 突触长时程可塑性 脉冲时间依赖可塑性（STDP） 图2-5显示了实验上观察到的脉冲时间依赖可塑性（spiking timing dependent plasticity；STDP）的现象。x轴为突触前神经元和突触后神经元产生脉冲（spike）的时间差，位于零点左侧的数据点为突触前神经元先于突触后神经元发放的情况，由图可见此时突触权重的改变量为正，表现出长时程增强 (long term potentiation；LTP）的现象；而零点右侧则是突触后神经元比突触前神经元更先发放的情况，表现出长时程抑制 (long term depression；LTD）。 图2-5 脉冲时间依赖可塑性 (改编自 Bi & Poo, 2001 2) STDP的计算模型如下： dAsdt=−Asτs \\frac {dA_s} {dt} = - \\frac {A_s} {\\tau_s} ​dt​​dA​s​​​​=−​τ​s​​​​A​s​​​​ dAtdt=−Atτt \\frac {dA_t} {dt} = - \\frac {A_t} {\\tau_t} ​dt​​dA​t​​​​=−​τ​t​​​​A​t​​​​ if (pre fire), then{s←s+wAs←As+ΔAsw←w−At \\text{if (pre fire), then} \\begin{cases} s \\leftarrow s + w \\\\ A_s \\leftarrow A_s + \\Delta A_s \\\\ w \\leftarrow w - A_t \\end{cases} if (pre fire), then​⎩​⎪​⎨​⎪​⎧​​​s←s+w​A​s​​←A​s​​+ΔA​s​​​w←w−A​t​​​​ if (post fire), then{At←At+ΔAtw←w+As \\text{if (post fire), then} \\begin{cases} A_t \\leftarrow A_t + \\Delta A_t \\\\ w \\leftarrow w + A_s \\end{cases} if (post fire), then{​A​t​​←A​t​​+ΔA​t​​​w←w+A​s​​​​ 其中www为突触权重，sss与上一节讨论的一样为门控变量。与STP模型类似，这里由AsA_{s}A​s​​和AtA_{t}A​t​​两个变量分别控制LTD和LTP。ΔAs\\Delta A_sΔA​s​​ 和 ΔAt\\Delta A_tΔA​t​​分别为AsA_{s}A​s​​ 和 AtA_{t}A​t​​的增量，而τs\\tau_sτ​s​​ 和 τt\\tau_tτ​t​​则分别为它们的时间常数。 根据这个模型，当突触前神经元先于突触后神经元发放时，在突触后神经元发放之前，每当突触前神经元有一个脉冲，AsA_sA​s​​便增加，而由于此时突触后神经元没有脉冲，因此AtA_tA​t​​保持在初始值0，www暂时不会有变化。直到突触后神经元发放时，www的增量将会是As−AtA_s - A_tA​s​​−A​t​​，由于As>AtA_s>A_tA​s​​>A​t​​，此时会表现出长时程增强（LTP）。反之亦然。 现在让我们看看如何使用BrainPy来实现这个模型。其中sss动力学的实现部分，我们跟STP模型一样采用单指数衰减模型。 我们通过给予突触前和突触后的两群神经元不同的电流输入来控制它们产生脉冲的时间。首先我们在t=5mst=5mst=5ms时刻给突触前神经元第一段电流（每一段强度为30 μA\\mu AμA，并持续15ms，保证LIF模型会产生一个脉冲），然后在t=10mst=10mst=10ms才给突触后神经元一个输入。每段输入之间间隔15ms15ms15ms。以此在前三对脉冲中保持tpost=tpre+5t_{post}=t_{pre}+5t​post​​=t​pre​​+5。接下来我们设置一个较长的间隔，然后把刺激顺序调整为tpost=tpre−3t_{post}=t_{pre}-3t​post​​=t​pre​​−3。 duration = 300. (I_pre, _) = bp.inputs.constant_current([(0, 5), (30, 15), # pre at 5ms (0, 15), (30, 15), (0, 15), (30, 15), (0, 98), (30, 15), # switch order: t_interval=98ms (0, 15), (30, 15), (0, 15), (30, 15), (0, duration-155-98)]) (I_post, _) = bp.inputs.constant_current([(0, 10), (30, 15), # post at 10 (0, 15), (30, 15), (0, 15), (30, 15), (0, 90), (30, 15), # switch order: t_interval=98-8=90(ms) (0, 15), (30, 15), (0, 15), (30, 15), (0, duration-160-90)]) 接下来跑模拟的代码和STP类似，这里我们画出突触前后神经元的脉冲时间以及sss和www随时间的变化。 pre = bm.neurons.LIF(1, monitors=['spike']) post = bm.neurons.LIF(1, monitors=['spike']) syn = STDP(pre=pre, post=post, conn=bp.connect.All2All(), monitors=['s', 'w']) net = bp.Network(pre, syn, post) net.run(duration, inputs=[(pre, 'input', I_pre), (post, 'input', I_post)]) # plot fig, gs = bp.visualize.get_figure(4, 1, 2, 7) def hide_spines(my_ax): plt.legend() plt.xticks([]) plt.yticks([]) my_ax.spines['left'].set_visible(False) my_ax.spines['right'].set_visible(False) my_ax.spines['bottom'].set_visible(False) my_ax.spines['top'].set_visible(False) ax=fig.add_subplot(gs[0, 0]) plt.plot(net.ts, syn.mon.s[:, 0], label=\"s\") hide_spines(ax) ax1=fig.add_subplot(gs[1, 0]) plt.plot(net.ts, pre.mon.spike[:, 0], label=\"pre spike\") plt.ylim(0, 2) hide_spines(ax1) plt.legend(loc = 'center right') ax2=fig.add_subplot(gs[2, 0]) plt.plot(net.ts, post.mon.spike[:, 0], label=\"post spike\") plt.ylim(-1, 1) hide_spines(ax2) ax3=fig.add_subplot(gs[3, 0]) plt.plot(net.ts, syn.mon.w[:, 0], label=\"w\") plt.legend() # hide spines plt.yticks([]) ax3.spines['left'].set_visible(False) ax3.spines['right'].set_visible(False) ax3.spines['top'].set_visible(False) plt.xlabel('Time (ms)') plt.show() 结果正如我们所预期的，在150ms前，突触前神经元的脉冲时间在突触后神经元之前，www增加，呈现LTP。而150ms后，突触后神经元先于突触前神经元发放，www减少，呈现LTD。 Oja法则 接下来我们看基于赫布学习律（Hebbian learning）的发放率模型 (firing rate model)。赫布学习律认为相互连接的两个神经元在经历同步的放电活动后，它们之间的突触连接就会得到增强。由于赫布学习律仅关心两个神经元的同步发放，而不关心发放的次序，可以用忽略具体的脉冲时间的发放率模型来实现。我们首先看赫布学习律的一般形式，对于神经元jjj到神经元iii的连接，用rjr_jr​j​​和rir_ir​i​​分别表示前神经元组和后神经元组的发放率，根据赫布学习律的局部性（locality）特性，wijw_{ij}w​ij​​的变化受www本身及rj,rir_j, r_ir​j​​,r​i​​的影响，得以下微分方程： ddtwij=F(wij;ri,rj) \\frac d {dt} w_{ij} = F(w_{ij}; r_{i},r_j) ​dt​​d​​w​ij​​=F(w​ij​​;r​i​​,r​j​​) 把上式右边经过泰勒展开可得下式： ddtwij=c00wij+c10wijrj+c01wijri+c20wijrj2+c02wijri2+c11wijrirj+O(r3) \\frac d {dt} w_{ij} = c_{00} w_{ij} + c_{10} w_{ij} r_j + c_{01} w_{ij} r_i + c_{20} w_{ij} r_j ^2 + c_{02} w_{ij} r_i ^2 + c_{11} w_{ij} r_i r_j + O(r^3) ​dt​​d​​w​ij​​=c​00​​w​ij​​+c​10​​w​ij​​r​j​​+c​01​​w​ij​​r​i​​+c​20​​w​ij​​r​j​2​​+c​02​​w​ij​​r​i​2​​+c​11​​w​ij​​r​i​​r​j​​+O(r​3​​) 赫布学习律的关键在于第六项，只有当第六项的系数c11c_{11}c​11​​非0才满足赫布学习律的同步发放。前面我们看了赫布学习律的一般形式，接下来我们看一个具体的例子。 Oja法则的公式如下，对应于上式第5、6项系数非零，其中γ\\gammaγ为学习速率（learning rate）。 ddtwij=γ[rirj−wijri2] \\frac d {dt} w_{ij} = \\gamma [r_i r_j - w_{ij} r_i ^2 ] ​dt​​d​​w​ij​​=γ[r​i​​r​j​​−w​ij​​r​i​2​​] 下面我们用BrainPy来实现Oja法则。 由于Oja法则是发放率模型，它需要突触前后神经元具有变量rrr，因此我们定义一个简单的发放率神经元模型来观察两组神经元的学习规则。 我们打算实现如图2-6所示的连接。突触后神经元群iii（紫色）同时接受两群神经元j1j_1j​1​​（蓝色）和j2j_2j​2​​（红色）的输入。我们给iii和j2j_2j​2​​完全相同的刺激，而给j1j_1j​1​​的刺激一开始跟iii一致，但后来就不一致了。因此，根据赫布学习律，我们预期同步发放时，突触权重www会增加，当j1j_1j​1​​不再与iii同步发放时，则wij1w_{ij_1}w​ij​1​​​​停止增加。 图2-6 神经元的连接 从结果可以看到，在前100ms内，j1j_1j​1​​和j2j_2j​2​​均与iii同步发放，他们对应的w1w_1w​1​​和w2w_2w​2​​也同步增加，显示出LTP。而100ms后，j1j_1j​1​​（蓝色）不再发放，只有j2j_2j​2​​（红色）与iii同步发放，因此w1w_1w​1​​不再增加，w2w_2w​2​​则持续增加。该结果符合赫布学习律。 BCM法则 现在我们来看赫布学习律的另一个例子——BCM法则。它的公式如下： ddtwij=ηri(ri−rθ)rj \\frac d{dt} w_{ij} = \\eta r_i(r_i - r_\\theta) r_j ​dt​​d​​w​ij​​=ηr​i​​(r​i​​−r​θ​​)r​j​​ 其中η\\etaη为学习速率，rθr_\\thetar​θ​​为学习的阈值（见图2-7）。图2-7画出了上式的右边，当发放频率高于阈值时呈现LTP，低于阈值时则为LTD。因此，这是一种频率依赖可塑性（回想一下，前面介绍的STDP为时间依赖可塑性），可以通过调整阈值rθr_\\thetar​θ​​实现选择性。 图2-7 BCM法则 (引自 Gerstner et al., 2014 1) 我们将实现和Oja法则相同的连接方式（图2-6），但给的刺激不同。在这里，我们让j1j_1j​1​​（蓝色）和j2j_2j​2​​（红色）交替发放，且j1j_1j​1​​的发放率比j2j_2j​2​​高。我们动态调整阈值为rir_ir​i​​的时间平均，即 rθ=f(ri)=∫dtriTr_\\theta = f(r_i)=\\frac {\\int dt r_i}Tr​θ​​=f(r​i​​)=​T​​∫dtr​i​​​​。BrainPy实现的代码如下。 定义了BCM类以后，我们可以跑模拟了。 # create input group1, _ = bp.inputs.constant_current(([1.5, 1], [0, 1]) * 10) group2, duration = bp.inputs.constant_current(([0, 1], [1., 1]) * 10) group1 = np.vstack(((group1,) * 10)) group2 = np.vstack(((group2,) * 10)) input_r = np.vstack((group1, group2)) # simulate pre = neu(20, monitors=['r']) post = neu(1, monitors=['r']) bcm = BCM(pre=pre, post=post, conn=bp.connect.All2All(), monitors=['w']) net = bp.Network(pre, bcm, post) net.run(duration, inputs=(pre, 'r', input_r.T, \"=\")) # plot fig, gs = bp.visualize.get_figure(2, 1) fig.add_subplot(gs[1, 0], xlim=(0, duration), ylim=(0, bcm.w_max)) plt.plot(net.ts, bcm.mon.w[:, 0], 'b', label='w1') plt.plot(net.ts, bcm.mon.w[:, 11], 'r', label='w2') plt.title(\"weights\") plt.ylabel(\"weights\") plt.xlabel(\"t\") plt.legend() fig.add_subplot(gs[0, 0], xlim=(0, duration)) plt.plot(net.ts, pre.mon.r[:, 0], 'b', label='r1') plt.plot(net.ts, pre.mon.r[:, 11], 'r', label='r2') plt.title(\"inputs\") plt.ylabel(\"firing rate\") plt.xlabel(\"t\") plt.legend() plt.show() 结果显示，每次发放率都比较高的j1j_1j​1​​（蓝色），其对应的w1w_1w​1​​持续增加，显示出LTP。而w2w_2w​2​​则呈现出LTD，这个结果显示出BCM法则的选择功能。 参考资料 [1] Gerstner, Wulfram, et al. Neuronal dynamics: From single neurons to networks and models of cognition. Cambridge University Press, 2014. [2] Bi, Guo-qiang, and Mu-ming Poo. \"Synaptic modification by correlated activity: Hebb's postulate revisited.\" Annual review of neuroscience 24.1 (2001): 139-166. "},"networks.html":{"url":"networks.html","title":"3. 网络模型","keywords":"","body":"3. 网络模型 到此，读者已经了解了几种最常见、最经典的神经元和突触模型，是时候更进一步了。本节中，我们将介绍计算神经科学中两种重要的网络模型：脉冲神经网络和发放率神经网络。 注：本章所述模型的完整BrainPy代码请见附录，或右键点此下载jupyter notebook版本。 3.1 脉冲神经网络 3.2 发放率神经网络 "},"networks/spiking_neural_networks.html":{"url":"networks/spiking_neural_networks.html","title":"3.1 脉冲神经网络","keywords":"","body":"3.1 脉冲神经网络 脉冲神经网络的特点是网络分别建模、计算每个神经元和突触。研究者希望通过这种仿真来观察大规模神经网络的行为，并验证相关理论推导。本节将介绍两个经典的脉冲神经网络模型：Vreeswijk和Sompolinsky（1996）提出的兴奋-抑制平衡网络和Wang（2002）提出的抉择网络。 3.1.1 兴奋-抑制平衡网络 上世纪90年代初，学界发现，在大脑皮层中神经元有时表现出一种在时间上不规则的发放特征。这种特征广泛地存在于脑区中，但当时人们对它的产生机制和主要功能都了解不多。 Vreeswijk和Sompolinsky（1996）提出了兴奋-抑制平衡网络（E/I balanced network），希望能够解释神经元这种不规则的发放，并提示了这种结构在功能上可能的优势。 图3-1 兴奋-抑制平衡网络结构 （Vreeswijk and Sompolinsky, 1996 1） 图3-1画出了兴奋-抑制平衡网络的结构。该网络由兴奋性LIF神经元和和抑制性LIF神经元构成，其数量比NE:NI=4:1N_E: N_I = 4:1N​E​​:N​I​​=4:1。在网络两类神经元之间和同类神经元之内，建立了四组指数型突触连接，分别是兴奋-兴奋连接（E2E conn），兴奋-抑制连接（E2I conn），抑制-兴奋连接（I2E conn），抑制-抑制连接（I2I conn）。在代码中我们通过定义符号相反的突触权重，来指明突触连接的兴奋性或抑制性。 注：LIF神经元和指数型突触的实现请参见第1节《神经元模型》和第2节《突触模型》 兴奋-抑制平衡网络在结构上最大的特征是神经元间强随机突触连接，连接概率为0.10.10.1，属于稀疏连接。 这种强的突触连接使得网络中每个神经元都会接收到很大的来自网络内部的兴奋性和抑制性输入。但是，这两种输入一正一负相互抵消，最后神经元接收到的总输入将保持在一个相对小的数量级上，仅足以让神经元的膜电位上升到阈值电位，引发其产生动作电位。 由于突触连接和噪声带来的随机性，网络中神经元接收到的输入也在时间和空间上具有一定的随机性（尽管总体保持在阈值电位量级上），这使得神经元的发放也具有随机性，保证兴奋-抑制平衡网络能够自发产生前述的时间上不规则的发放特征。 下述仿真结果中，可以看到网络中的神经元从一开始的强同步发放慢慢变为时间上不规则的发放。 与此同时，作者还提出了这种发放特征在大脑中可能提供的功能：兴奋-抑制平衡网络可以快速跟踪外部刺激的变化。假如该网络真的是大脑中神经元产生不规则发放背后的机制，那么真实的神经元网络也可能拥有同样的特性。 如图3-2所示，当没有外部输入时，兴奋-抑制平衡网络中神经元的膜电位相对均匀且随机地分布在静息电位V0V_0V​0​​和阈值电位θ\\thetaθ之间。当网络接收到一个小的外部恒定输入时，那些膜电位原本就落在阈值电位附近的神经元（图中标为红色）就能很快地发放，在网络尺度上，表现为网络的发放率随输入变化而快速改变。 图3-2 兴奋-抑制平衡网络中神经元膜电位的分布（Tian et al.，2020 2） 仿真证实，在这种情况下，网络对输入产生反应的延迟时间和突触的延迟时间处于同一量级，并且二者都远小于单神经元从静息电位开始积累同样大小的外部输入直到产生动作电位所需的延迟时间（Vreeswijk和Sompolinsky，1996；Tian et al.，2020）。因此，兴奋-抑制平衡网络面对外部输入的变化可以快速反应，改变自身的活跃水平。 3.1.2 抉择网络 计算神经科学的网络建模也可以对标特定的生理实验任务，比如视觉运动区分实验（Parker和Newsome，1998；Roitman和Shadlen，2002）。 在该实验中，参与实验的猕猴将观看一段随机点的运动展示。在展示过程中，随机点以一定比例（该比例被定义为一致度（coherence））向特定方向运动，其他点则向随机方向运动。猕猴被要求判断随机点一致运动的方向，并通过眼动给出答案。同时，研究者通过电生理手段记录猕猴LIP神经元的活动。 图3-3 生理实验中随机点的运动示意图（Gerstner et al., 20143） Wang（2002）提出了本节所述的抉择网络，希望建模在视觉运动区分实验中猕猴大脑新皮层的抉择回路的活动。 图3-4 抉择网络结构（Wang，20024） 如图3-4所示，网络同样基于兴奋-抑制平衡网络。兴奋性神经元和抑制型神经元的数量比是NE:NI=4:1N_E:N_I = 4:1N​E​​:N​I​​=4:1，调整参数使得网络处在平衡状态下。 为了简化模型，实验被设定为一个二选一的任务：在兴奋性神经元群中，特别地标出两个选择性子神经元群A和B，其他的兴奋性神经元称为非选择性神经元，用下标non_{non}​non​​表示。A群和B群的数目均为兴奋性神经元的0.15倍（NA=NB=0.15NEN_A = N_B = 0.15N_EN​A​​=N​B​​=0.15N​E​​），它们分别代表着两个相反的运动方向，可以视作随机点要么向左，要么向右，没有第三个方向，网络的抉择结果也必须在这两个子群中产生。非选择性神经元的数目为Nnon=(1−2∗0.15)NEN_{non} = (1-2*0.15)N_EN​non​​=(1−2∗0.15)N​E​​。 抉择网络中共有四组突触——E2E，E2I，I2E和I2I突触连接，其中兴奋性突触实现为AMPA突触，抑制性突触实现为GABAa突触。 由于网络需要在A群和B群之间作出抉择，所以这两个子神经元群之间必须形成一种竞争关系。一个选择性子神经元群应当激活自身，并同时抑制另一个选择性子神经元群。 因此，网络中的E2E连接被建模为有结构的连接。如表3-1所示，w+>1>w−w+ > 1 > w-w+>1>w−。这样，在A群或B群的内部，兴奋性突触连接更强，形成了一种相对的自激活；而在A、B两个选择性子神经元群之间或是A群、B群和非选择性子神经元群之间，兴奋性突触连接较弱，实际上形成了相对的抑制。A和B两个神经元因此产生竞争，迫使网络做出二选一的抉择。 表3-1 决策网络中兴奋性神经元间连接权重的分布 抉择网络接受的外部输入可分为两类： 1）所有神经元都收到从其他脑区传来的非特定的背景输入，表示为AMPA突触介导的高频泊松输入（2400Hz）。 2）仅选择性的A群和B群收到外部传来的刺激输入，表示为AMPA突触介导的较低频泊松输入（约100Hz内）。 给予A和B神经元群的泊松输入的频率均值（μA\\mu_Aμ​A​​、μB\\mu_Bμ​B​​）有一定差别，对应到生理实验上，代表猕猴看到的随机点朝两个相反方向运动的比例（用coherence表示）不同。这种输入上的差别引导着网络在两个子神经元群中做出抉择。 ρA=ρB=μ0/100 \\rho_A = \\rho_B = \\mu_0/100 ρ​A​​=ρ​B​​=μ​0​​/100 μA=μ0+ρA∗coherence \\mu_A = \\mu_0 + \\rho_A * coherence μ​A​​=μ​0​​+ρ​A​​∗coherence μB=μ0+ρB∗coherence \\mu_B = \\mu_0 + \\rho_B * coherence μ​B​​=μ​0​​+ρ​B​​∗coherence 每50毫秒，泊松输入的实际频率fxf_xf​x​​遵循由均值μx\\mu_xμ​x​​ 和方差δ2\\delta^2δ​2​​定义的高斯分布，重新进行一次采样。 fA∼N(μA,δ2) f_A \\sim N(\\mu_A, \\delta^2) f​A​​∼N(μ​A​​,δ​2​​) fB∼N(μB,δ2) f_B \\sim N(\\mu_B, \\delta^2) f​B​​∼N(μ​B​​,δ​2​​) 下图中可以看到，在本次仿真中，子神经元群A收到的刺激输入平均大于B收到的刺激输入。经过一定的延迟时间，A群的活动水平明显高于B群，说明网络做出了正确的选择。 参考资料 [1] Van Vreeswijk, Carl, and Haim Sompolinsky. \"Chaos in neuronal networks with balanced excitatory and inhibitory activity.\" Science 274.5293 (1996): 1724-1726. [2] Tian, Gengshuo, et al. \"Excitation-Inhibition Balanced Neural Networks for Fast Signal Detection.\" Frontiers in Computational Neuroscience 14 (2020): 79. [3] Gerstner, Wulfram, et al. Neuronal dynamics: From single neurons to networks and models of cognition. Cambridge University Press, 2014. [4] Wang, Xiao-Jing. \"Probabilistic decision making by slow reverberation in cortical circuits.\" Neuron 36.5 (2002): 955-968. "},"networks/rate_models.html":{"url":"networks/rate_models.html","title":"3.2 发放率神经网络","keywords":"","body":"3.2 发放率神经网络 3.2.1 抉择模型 我们在上一节中介绍了Wang（2002）提出的抉择模型，现在来介绍他们后续做的一个基于发放率（firing rate）的简化模型（Wong & Wang, 20061）。该模型的实验背景与上一节的相同，在脉冲神经网络模型的基础上，他们使用平均场近似（mean-field approach）等方法，使用一群神经元的发放率来表示整群神经元的状态，而不再关注每个神经元的脉冲。他们拟合出输入-输出函数（input-output function）来表示给一群神经元一个外界输入电流III时，这群神经元的发放率rrr如何改变，即r=f(I)r=f(I)r=f(I)。经过这样的简化后，我们就可以很方便地对其进行动力学分析。 图3-5 简化的抉择模型 (引自 Wong & Wang, 2006 1) 基于发放率的抉择模型如图3-5所示，S1S_1S​1​​（蓝色）和S2S_2S​2​​（红色）分别表示两群神经元的状态，同时也分别对应着两个选项。他们都由兴奋性的神经元组成，且各自都有一个循环（recurrent）连接。而同时它们都会给对方一个抑制性的输入，以此形成相互竞争的关系。该模型的动力学方程如下： dS1dt=−S1τ+(1−S1)γr1 \\frac{dS_1} {dt} = -\\frac {S_1} \\tau + (1-S_1) \\gamma r_1 ​dt​​dS​1​​​​=−​τ​​S​1​​​​+(1−S​1​​)γr​1​​ dS2dt=−S2τ+(1−S2)γr2 \\frac{dS_2} {dt} = -\\frac {S_2} \\tau + (1-S_2) \\gamma r_2 ​dt​​dS​2​​​​=−​τ​​S​2​​​​+(1−S​2​​)γr​2​​ 其中τ\\tauτ为时间常数，γ\\gammaγ为拟合得到的常数， r1r_1r​1​​ 和 r2r_2r​2​​ 分别为两群神经元的发放率，其输入-输出函数为： ri=f(Isyn,i) r_i = f(I_{syn, i}) r​i​​=f(I​syn,i​​) f(I)=aI−b1−exp[−d(aI−b)] f(I)= \\frac {aI-b} {1- \\exp [-d(aI-b)]} f(I)=​1−exp[−d(aI−b)]​​aI−b​​ Isyn,iI_{syn, i}I​syn,i​​ 的公式由图3-5的模型结构给出： Isyn,1=J11S1−J12S2+I0+I1 I_{syn, 1} = J_{11} S_1 - J_{12} S_2 + I_0 + I_1 I​syn,1​​=J​11​​S​1​​−J​12​​S​2​​+I​0​​+I​1​​ Isyn,2=J22S2−J21S1+I0+I2 I_{syn, 2} = J_{22} S_2 - J_{21} S_1 + I_0 + I_2 I​syn,2​​=J​22​​S​2​​−J​21​​S​1​​+I​0​​+I​2​​ 其中I0I_0I​0​​为背景电流，外界输入 I1,I2I_1, I_2I​1​​,I​2​​ 则由总输入的强度 μ0\\mu_0μ​0​​ 及一致性（coherence） c′c'c​′​​ 决定。一致性越高，则越明确S1S_1S​1​​是正确答案，而一致性越低则表示越随机。公式如下： I1=JA, extμ0(1+c′100%) I_1 = J_{\\text{A, ext}} \\mu_0 (1+\\frac {c'}{100\\%}) I​1​​=J​A, ext​​μ​0​​(1+​100%​​c​′​​​​) I2=JA, extμ0(1−c′100%) I_2 = J_{\\text{A, ext}} \\mu_0 (1-\\frac {c'}{100\\%}) I​2​​=J​A, ext​​μ​0​​(1−​100%​​c​′​​​​) 接下来，我们将继承bp.NeuGroup类，并用BrainPy提供的相平面分析方法bp.analysis.PhasePlane进行动力学分析。首先，我们把上面的动力学公式写到一个derivative函数中，定义一个Decision类（本章代码请查看附录，或点此下载jupyter notebook）。 接下来，我们想要看模型在不同输入情况下的动力学，因此，我们先定义一个对抉择模型做相平面分析的方法，可以让我们改变I（即外界输入强度μ0\\mu_0μ​0​​）和coh（即输入的一致性c′c'c​′​​），而固定了参数的值等。 现在让我们来看看当没有外界输入，即μ0=0\\mu_0 = 0μ​0​​=0时的动力学。 phase_analyze(I=0., coh=0.) plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.06176109215560733, s1=0.061761097890810475 is a stable node. Fixed point #2 at s2=0.029354239100062428, s1=0.18815448592736211 is a saddle node. Fixed point #3 at s2=0.0042468423702408655, s1=0.6303045696241589 is a stable node. Fixed point #4 at s2=0.6303045696241589, s1=0.004246842370235128 is a stable node. Fixed point #5 at s2=0.18815439944520335, s1=0.029354240536530615 is a saddle node. plot vector field ... 由此可见，用BrainPy进行动力学分析是非常方便的。向量场和不动点 (fixed point)表示了不同初始值下最终会落在哪个选项。 这里，x轴是S2S_2S​2​​，代表选项2，y轴是S1S_1S​1​​，代表选项1。可以看到，左上的不动点表示选项1，右下的不动点表示选项2，左下的不动点表示没有选择。 现在让我们看看当我们把外部输入强度固定为30时，在不同一致性（coherence）下的相平面。 # coherence = 0% print(\"coherence = 0%\") phase_analyze(I=30., coh=0.) # coherence = 51.2% print(\"coherence = 51.2%\") phase_analyze(I=30., coh=0.512) # coherence = 100% print(\"coherence = 100%\") phase_analyze(I=30., coh=1.) coherence = 0% plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.6993504413889349, s1=0.011622049526766405 is a stable node. Fixed point #2 at s2=0.49867489858358865, s1=0.49867489858358865 is a saddle node. Fixed point #3 at s2=0.011622051540013889, s1=0.6993504355529329 is a stable node. plot vector field ... coherence = 51.2% plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.5673124813731691, s1=0.2864701069327971 is a saddle node. Fixed point #2 at s2=0.6655747347157656, s1=0.027835279565912054 is a stable node. Fixed point #3 at s2=0.005397687847426814, s1=0.7231453520305031 is a stable node. plot vector field ... coherence = 100% plot nullcline ... plot fixed point ... Fixed point #1 at s2=0.0026865954387078755, s1=0.7410985604497689 is a stable node. plot vector field ... 3.2.2 连续吸引子模型（CANN） 这里我们将介绍发放率模型的另一个例子——连续吸引子神经网络（CANN）。图3-6呈现了一维CANN的结构。 图3-6 连续吸引子神经网络 (引自 Wu et al., 2008 2) 神经元的突触总输入uuu的动力学方程如下： τdu(x,t)dt=−u(x,t)+ρ∫dx′J(x,x′)r(x′,t)+Iext \\tau \\frac{du(x,t)}{dt} = -u(x,t) + \\rho \\int dx' J(x,x') r(x',t)+I_{ext} τ​dt​​du(x,t)​​=−u(x,t)+ρ∫dx​′​​J(x,x​′​​)r(x​′​​,t)+I​ext​​ 其中x表示神经元的参数空间位点，r(x′,t)r(x', t)r(x​′​​,t)为神经元(x')的发放率，由以下公式给出: r(x,t)=u(x,t)21+kρ∫dx′u(x′,t)2 r(x,t) = \\frac{u(x,t)^2}{1 + k \\rho \\int dx' u(x',t)^2} r(x,t)=​1+kρ∫dx​′​​u(x​′​​,t)​2​​​​u(x,t)​2​​​​ 而神经元(x)和(x')之间的兴奋性连接强度J(x,x′)J(x, x')J(x,x​′​​)由高斯函数给出: J(x,x′)=12πaexp(−∣x−x′∣22a2) J(x,x') = \\frac{1}{\\sqrt{2\\pi}a}\\exp(-\\frac{|x-x'|^2}{2a^2}) J(x,x​′​​)=​√​2π​​​a​​1​​exp(−​2a​2​​​​∣x−x​′​​∣​2​​​​) 外界输入IextI_{ext}I​ext​​与位置z(t)z(t)z(t)有关，公式如下： Iext=Aexp[−∣x−z(t)∣24a2] I_{ext} = A\\exp\\left[-\\frac{|x-z(t)|^2}{4a^2}\\right] I​ext​​=Aexp[−​4a​2​​​​∣x−z(t)∣​2​​​​] 用BrainPy实现的代码如下，我们通过继承bp.NeuGroup来创建一个CANN1D的类。 这里我们用函数dist与make_conn来计算两群神经元之间的连接强度J(x,x′)J(x, x')J(x,x​′​​)。其中dist函数用来处理环上的距离。 接下来我们可以调用刚才定义的get_stimulus_by_pos方法获取外界输入电流大小。例如在简单的群体编码（population coding）中，我们给一个pos=0的外界输入，并按以下方式运行： cann = CANN1D(num=512, k=0.1, monitors=['u']) I1 = cann.get_stimulus_by_pos(0.) Iext, duration = bp.inputs.constant_current([(0., 1.), (I1, 8.), (0., 8.)]) cann.run(duration=duration, inputs=('input', Iext)) 我们写一个plot_animate的函数来方便重复调用bp.visualize.animate_1D画结果图。 # 定义函数 def plot_animate(frame_step=5, frame_delay=50): bp.visualize.animate_1D(dynamical_vars=[{'ys': cann.mon.u, 'xs': cann.x, 'legend': 'u'}, {'ys': Iext, 'xs': cann.x, 'legend': 'Iext'}], frame_step=frame_step, frame_delay=frame_delay, show=True) # 调用函数 plot_animate(frame_step=1, frame_delay=100) 可以看到，uuu的形状编码了外界输入的形状。 现在我们给外界输入加上随机噪声，看看uuu的形状如何变化。 cann = CANN1D(num=512, k=8.1, monitors=['u']) dur1, dur2, dur3 = 10., 30., 0. num1 = int(dur1 / bp.backend.get_dt()) num2 = int(dur2 / bp.backend.get_dt()) num3 = int(dur3 / bp.backend.get_dt()) Iext = np.zeros((num1 + num2 + num3,) + cann.size) Iext[:num1] = cann.get_stimulus_by_pos(0.5) Iext[num1:num1 + num2] = cann.get_stimulus_by_pos(0.) Iext[num1:num1 + num2] += 0.1 * cann.A * np.random.randn(num2, *cann.size) cann.run(duration=dur1 + dur2 + dur3, inputs=('input', Iext)) plot_animate() 我们可以看到uuu的形状保持一个类似高斯的钟形，这表明CANN可以进行模板匹配。 接下来我们用np.linspace函数来产生不同的位置，得到随时间平移的输入，我们将会看到uuu跟随着外界输入移动，即平滑跟踪。 cann = CANN1D(num=512, k=8.1, monitors=['u']) dur1, dur2, dur3 = 20., 20., 20. num1 = int(dur1 / bp.backend.get_dt()) num2 = int(dur2 / bp.backend.get_dt()) num3 = int(dur3 / bp.backend.get_dt()) position = np.zeros(num1 + num2 + num3) position[num1: num1 + num2] = np.linspace(0., 12., num2) position[num1 + num2:] = 12. position = position.reshape((-1, 1)) Iext = cann.get_stimulus_by_pos(position) cann.run(duration=dur1 + dur2 + dur3, inputs=('input', Iext)) plot_animate() 参考资料 [1] Wong, K.-F. & Wang, X.-J. A Recurrent Network Mechanism of Time Integration in Perceptual Decisions. J. Neurosci. 26, 1314–1328 (2006). [2] Si Wu, Kosuke Hamaguchi, and Shun-ichi Amari. \"Dynamics and computation of continuous attractors.\" Neural computation 20.4 (2008): 994-1025. "},"appendix/neurons.html":{"url":"appendix/neurons.html","title":"神经元模型","keywords":"","body":"生理模型 Hodgkin-Huxley模型 import brainpy as bp from numba import prange class HH(bp.NeuGroup): target_backend = 'general' @staticmethod @bp.odeint(method='exponential_euler') def integral(V, m, h, n, t, C, gNa, ENa, gK, EK, gL, EL, Iext): alpha_m = 0.1*(V+40)/(1-bp.ops.exp(-(V+40)/10)) beta_m = 4.0*bp.ops.exp(-(V+65)/18) dmdt = alpha_m * (1 - m) - beta_m * m alpha_h = 0.07*bp.ops.exp(-(V+65)/20) beta_h = 1/(1+bp.ops.exp(-(V+35)/10)) dhdt = alpha_h * (1 - h) - beta_h * h alpha_n = 0.01*(V+55)/(1-bp.ops.exp(-(V+55)/10)) beta_n = 0.125*bp.ops.exp(-(V+65)/80) dndt = alpha_n * (1 - n) - beta_n * n I_Na = (gNa * m ** 3.0 * h) * (V - ENa) I_K = (gK * n ** 4.0) * (V - EK) I_leak = gL * (V - EL) dVdt = (- I_Na - I_K - I_leak + Iext) / C return dVdt, dmdt, dhdt, dndt def __init__(self, size, ENa=50., gNa=120., EK=-77., gK=36., EL=-54.387, gL=0.03, V_th=20., C=1.0, **kwargs): # parameters self.ENa = ENa self.EK = EK self.EL = EL self.gNa = gNa self.gK = gK self.gL = gL self.C = C self.V_th = V_th # variables num = bp.size2len(size) self.V = -65. * bp.ops.ones(num) self.m = 0.5 * bp.ops.ones(num) self.h = 0.6 * bp.ops.ones(num) self.n = 0.32 * bp.ops.ones(num) self.spike = bp.ops.zeros(num, dtype=bool) self.input = bp.ops.zeros(num) super(HH, self).__init__(size=size, **kwargs) def update(self, _t): V, m, h, n = self.integral(self.V, self.m, self.h, self.n, _t, self.C, self.gNa, self.ENa, self.gK, self.EK, self.gL, self.EL, self.input) self.spike = (self.V = self.V_th) self.V = V self.m = m self.h = h self.n = n self.input[:] = 0 import brainpy as bp dt = 0.1 bp.backend.set('numpy', dt=dt) neu = HH(100, monitors=['V', 'spike']) neu.t_refractory = 5. net = bp.Network(neu) net.run(duration=200., inputs=(neu, 'input', 21.), report=True) fig, gs = bp.visualize.get_figure(1, 1, 4, 10) fig.add_subplot(gs[0, 0]) bp.visualize.line_plot(neu.mon.ts, neu.mon.V, xlabel=\"t\", ylabel=\"V\", show=True) 简化模型 LIF模型 import brainpy as bp from numba import prange class LIF(bp.NeuGroup): target_backend = ['numpy', 'numba', 'numba-parallel', 'numba-cuda'] @staticmethod def derivative(V, t, Iext, V_rest, R, tau): dvdt = (-V + V_rest + R * Iext) / tau return dvdt def __init__(self, size, t_refractory=1., V_rest=0., V_reset=-5., V_th=20., R=1., tau=10., **kwargs): # parameters self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.R = R self.tau = tau self.t_refractory = t_refractory # variables num = bp.size2len(size) self.t_last_spike = bp.ops.ones(num) * -1e7 self.input = bp.ops.zeros(num) self.refractory = bp.ops.zeros(num, dtype=bool) self.spike = bp.ops.zeros(num, dtype=bool) self.V = bp.ops.ones(num) * V_rest self.integral = bp.odeint(self.derivative) super(LIF, self).__init__(size=size, **kwargs) def update(self, _t): for i in prange(self.size[0]): spike = 0. refractory = (_t - self.t_last_spike[i] = self.V_th) if spike: V = self.V_reset self.t_last_spike[i] = _t self.V[i] = V self.spike[i] = spike self.refractory[i] = refractory or spike self.input[i] = 0. import brainpy as bp dt = 0.1 bp.backend.set('numpy', dt=dt) neu = LIF(100, monitors=['V', 'refractory', 'spike']) neu.t_refractory = 5. net = bp.Network(neu) net.run(duration=200., inputs=(neu, 'input', 21.), report=True) fig, gs = bp.visualize.get_figure(1, 1, 4, 10) fig.add_subplot(gs[0, 0]) bp.visualize.line_plot(neu.mon.ts, neu.mon.V, xlabel=\"t\", ylabel=\"V\", show=True) QuaIF模型 import brainpy as bp from numba import prange class QuaIF(bp.NeuGroup): target_backend = 'general' @staticmethod def derivative(V, t, I_ext, V_rest, V_c, R, tau, a_0): dVdt = (a_0 * (V - V_rest) * (V - V_c) + R * I_ext) / tau return dVdt def __init__(self, size, V_rest=-65., V_reset=-68., V_th=-30., V_c=-50.0, a_0=.07, R=1., tau=10., t_refractory=0., **kwargs): # parameters self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.V_c = V_c self.a_0 = a_0 self.R = R self.tau = tau self.t_refractory = t_refractory # variables num = bp.size2len(size) self.V = bp.ops.ones(num) * V_reset self.input = bp.ops.zeros(num) self.spike = bp.ops.zeros(num, dtype=bool) self.refractory = bp.ops.zeros(num, dtype=bool) self.t_last_spike = bp.ops.ones(num) * -1e7 self.integral = bp.odeint(f=self.derivative, method='euler') super(QuaIF, self).__init__(size=size, **kwargs) def update(self, _t): for i in prange(self.size[0]): spike = 0. refractory = (_t - self.t_last_spike[i] = self.V_th) if spike: V = self.V_rest self.t_last_spike[i] = _t self.V[i] = V self.spike[i] = spike self.refractory[i] = refractory or spike self.input[i] = 0. dt = 0.1 bp.backend.set('numpy', dt=dt) neu = QuaIF(100, monitors=['V', 'refractory', 'spike']) neu.t_refractory = 5. net = bp.Network(neu) net.run(duration=200., inputs=(neu, 'input', 21.), report=True) fig, gs = bp.visualize.get_figure(1, 1, 4, 10) fig.add_subplot(gs[0, 0]) bp.visualize.line_plot(neu.mon.ts, neu.mon.V, xlabel=\"t\", ylabel=\"V\", show=True) ExpIF模型 import brainpy as bp from numba import prange class ExpIF(bp.NeuGroup): target_backend = 'general' @staticmethod def derivative(V, t, I_ext, V_rest, delta_T, V_T, R, tau): exp_term = bp.ops.exp((V - V_T) / delta_T) dvdt = (-(V-V_rest) + delta_T*exp_term + R*I_ext) / tau return dvdt def __init__(self, size, V_rest=-65., V_reset=-68., V_th=-30., V_T=-59.9, delta_T=3.48, R=10., C=1., tau=10., t_refractory=1.7, **kwargs): # parameters self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.V_T = V_T self.delta_T = delta_T self.R = R self.C = C self.tau = tau self.t_refractory = t_refractory # variables self.V = bp.ops.ones(size) * V_rest self.input = bp.ops.zeros(size) self.spike = bp.ops.zeros(size, dtype=bool) self.refractory = bp.ops.zeros(size, dtype=bool) self.t_last_spike = bp.ops.ones(size) * -1e7 self.integral = bp.odeint(self.derivative) super(ExpIF, self).__init__(size=size, **kwargs) def update(self, _t): for i in prange(self.num): spike = 0. refractory = (_t - self.t_last_spike[i] = self.V_th) if spike: V = self.V_reset self.t_last_spike[i] = _t self.V[i] = V self.spike[i] = spike self.refractory[i] = refractory or spike self.input[:] = 0. dt = 0.1 bp.backend.set('numpy', dt=dt) neu = ExpIF(100, monitors=['V', 'refractory', 'spike']) neu.t_refractory = 5. net = bp.Network(neu) net.run(duration=200., inputs=(neu, 'input', 21.), report=True) fig, gs = bp.visualize.get_figure(1, 1, 4, 10) fig.add_subplot(gs[0, 0]) bp.visualize.line_plot(neu.mon.ts, neu.mon.V, xlabel=\"t\", ylabel=\"V\", show=True) AdExIF模型 import brainpy as bp from numba import prange class AdExIF(bp.NeuGroup): target_backend = 'general' @staticmethod def derivative(V, w, t, I_ext, V_rest, delta_T, V_T, R, tau, tau_w, a): exp_term = bp.ops.exp((V-V_T)/delta_T) dVdt = (-(V-V_rest)+delta_T*exp_term-R*w+R*I_ext)/tau dwdt = (a*(V-V_rest)-w)/tau_w return dVdt, dwdt def __init__(self, size, V_rest=-65., V_reset=-68., V_th=-30., V_T=-59.9, delta_T=3.48, a=1., b=1., R=10., tau=10., tau_w=30., t_refractory=0., **kwargs): # parameters self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.V_T = V_T self.delta_T = delta_T self.a = a self.b = b self.R = R self.tau = tau self.tau_w = tau_w self.t_refractory = t_refractory # variables num = bp.size2len(size) self.V = bp.ops.ones(num) * V_reset self.w = bp.ops.zeros(size) self.input = bp.ops.zeros(num) self.spike = bp.ops.zeros(num, dtype=bool) self.refractory = bp.ops.zeros(num, dtype=bool) self.t_last_spike = bp.ops.ones(num) * -1e7 self.integral = bp.odeint(f=self.derivative, method='euler') super(AdExIF, self).__init__(size=size, **kwargs) def update(self, _t): for i in prange(self.size[0]): spike = 0. refractory = (_t - self.t_last_spike[i] = self.V_th) if spike: V = self.V_rest w += self.b self.t_last_spike[i] = _t self.V[i] = V self.w[i] = w self.spike[i] = spike self.refractory[i] = refractory or spike self.input[i] = 0. dt = 0.1 bp.backend.set('numpy', dt=dt) neu = AdExIF(100, monitors=['V', 'refractory', 'spike']) neu.t_refractory = 5. net = bp.Network(neu) net.run(duration=200., inputs=(neu, 'input', 21.), report=True) fig, gs = bp.visualize.get_figure(1, 1, 4, 10) fig.add_subplot(gs[0, 0]) bp.visualize.line_plot(neu.mon.ts, neu.mon.V, xlabel=\"t\", ylabel=\"V\", show=True) Hindmarsh-Rose模型 import brainpy as bp from numba import prange class HindmarshRose(bp.NeuGroup): target_backend = 'general' @staticmethod def derivative(V, y, z, t, a, b, I_ext, c, d, r, s, V_rest): dVdt = y - a * V * V * V + b * V * V - z + I_ext dydt = c - d * V * V - y dzdt = r * (s * (V - V_rest) - z) return dVdt, dydt, dzdt def __init__(self, size, a=1., b=3., c=1., d=5., r=0.01, s=4., V_rest=-1.6, **kwargs): # parameters self.a = a self.b = b self.c = c self.d = d self.r = r self.s = s self.V_rest = V_rest # variables num = bp.size2len(size) self.z = bp.ops.zeros(num) self.input = bp.ops.zeros(num) self.V = bp.ops.ones(num) * -1.6 self.y = bp.ops.ones(num) * -10. self.spike = bp.ops.zeros(num, dtype=bool) self.integral = bp.odeint(f=self.derivative) super(HindmarshRose, self).__init__(size=size, **kwargs) def update(self, _t): for i in prange(self.num): V, self.y[i], self.z[i] = self.integral( self.V[i], self.y[i], self.z[i], _t, self.a, self.b, self.input[i], self.c, self.d, self.r, self.s, self.V_rest) self.V[i] = V self.input[i] = 0. bp.backend.set('numba', dt=0.02) mode = 'irregular_bursting' param = {'quiescence': [1.0, 2.0], # a 'spiking': [3.5, 5.0], # c 'bursting': [2.5, 3.0], # d 'irregular_spiking': [2.95, 3.3], # h 'irregular_bursting': [2.8, 3.7], # g } # set params of b and I_ext corresponding to different firing mode print(f\"parameters is set to firing mode \") group = HindmarshRose(size=10, b=param[mode][0], monitors=['V', 'y', 'z']) group.run(350., inputs=('input', param[mode][1]), report=True) bp.visualize.line_plot(group.mon.ts, group.mon.V, show=True) # Phase plane analysis phase_plane_analyzer = bp.analysis.PhasePlane( neu.integral, target_vars={'V': [-3., 3.], 'y': [-20., 5.]}, fixed_vars={'z': 0.}, pars_update={'I_ext': param[mode][1], 'a': 1., 'b': 3., 'c': 1., 'd': 5., 'r': 0.01, 's': 4., 'V_rest': -1.6} ) phase_plane_analyzer.plot_nullcline() phase_plane_analyzer.plot_fixed_point() phase_plane_analyzer.plot_vector_field() phase_plane_analyzer.plot_trajectory( [{'V': 1., 'y': 0., 'z': -0.}], duration=100., show=True ) GeneralizedIF模型 import brainpy as bp from numba import prange class GeneralizedIF(bp.NeuGroup): target_backend = 'general' @staticmethod def derivative(I1, I2, V_th, V, t, k1, k2, a, V_rest, b, V_th_inf, R, I_ext, tau): dI1dt = - k1 * I1 dI2dt = - k2 * I2 dVthdt = a * (V - V_rest) - b * (V_th - V_th_inf) dVdt = (- (V - V_rest) + R * I_ext + R * I1 + R * I2) / tau return dI1dt, dI2dt, dVthdt, dVdt def __init__(self, size, V_rest=-70., V_reset=-70., V_th_inf=-50., V_th_reset=-60., R=20., tau=20., a=0., b=0.01, k1=0.2, k2=0.02, R1=0., R2=1., A1=0., A2=0., **kwargs): # params self.V_rest = V_rest self.V_reset = V_reset self.V_th_inf = V_th_inf self.V_th_reset = V_th_reset self.R = R self.tau = tau self.a = a self.b = b self.k1 = k1 self.k2 = k2 self.R1 = R1 self.R2 = R2 self.A1 = A1 self.A2 = A2 # vars self.input = bp.ops.zeros(size) self.spike = bp.ops.zeros(size, dtype=bool) self.I1 = bp.ops.zeros(size) self.I2 = bp.ops.zeros(size) self.V = bp.ops.ones(size) * -70. self.V_th = bp.ops.ones(size) * -50. self.integral = bp.odeint(self.derivative) super(GeneralizedIF, self).__init__(size=size, **kwargs) def update(self, _t): for i in prange(self.size[0]): I1, I2, V_th, V = self.integral( self.I1[i], self.I2[i], self.V_th[i], self.V[i], _t, self.k1, self.k2, self.a, self.V_rest, self.b, self.V_th_inf, self.R, self.input[i], self.tau ) self.spike[i] = self.V_th[i] 发放率模型 发放率单元 import brainpy as bp from numba import prange class FiringRateUnit(bp.NeuGroup): target_backend = 'general' @staticmethod def derivative(a_e, a_i, t, k_e, r_e, c1, c2, I_ext_e, slope_e, theta_e, tau_e, k_i, r_i, c3, c4, I_ext_i, slope_i, theta_i, tau_i): x_ae = c1 * a_e - c2 * a_i + I_ext_e sigmoid_ae_l = 1 / (1 + bp.ops.exp(- slope_e * (x_ae - theta_e))) sigmoid_ae_r = 1 / (1 + bp.ops.exp(slope_e * theta_e)) sigmoid_ae = sigmoid_ae_l - sigmoid_ae_r daedt = (- a_e + (k_e - r_e * a_e) * sigmoid_ae) / tau_e x_ai = c3 * a_e - c4 * a_i + I_ext_i sigmoid_ai_l = 1 / (1 + bp.ops.exp(- slope_i * (x_ai - theta_i))) sigmoid_ai_r = 1 / (1 + bp.ops.exp(slope_i * theta_i)) sigmoid_ai = sigmoid_ai_l - sigmoid_ai_r daidt = (- a_i + (k_i - r_i * a_i) * sigmoid_ai) / tau_i return daedt, daidt def __init__(self, size, c1=12., c2=4., c3=13., c4=11., k_e=1., k_i=1., tau_e=1., tau_i=1., r_e=1., r_i=1., slope_e=1.2, slope_i=1., theta_e=2.8, theta_i=4., **kwargs): # params self.c1 = c1 self.c2 = c2 self.c3 = c3 self.c4 = c4 self.k_e = k_e self.k_i = k_i self.tau_e = tau_e self.tau_i = tau_i self.r_e = r_e self.r_i = r_i self.slope_e = slope_e self.slope_i = slope_i self.theta_e = theta_e self.theta_i = theta_i # vars self.input_e = bp.backend.zeros(size) self.input_i = bp.backend.zeros(size) self.a_e = bp.backend.ones(size) * 0.1 self.a_i = bp.backend.ones(size) * 0.05 self.integral = bp.odeint(self.derivative) super(FiringRateUnit, self).__init__(size=size, **kwargs) def update(self, _t): self.a_e, self.a_i = self.integral( self.a_e, self.a_i, _t, self.k_e, self.r_e, self.c1, self.c2, self.input_e, self.slope_e, self.theta_e, self.tau_e, self.k_i, self.r_i, self.c3, self.c4, self.input_i, self.slope_i, self.theta_i, self.tau_i) self.input_e[:] = 0. self.input_i[:] = 0. "},"appendix/synapses.html":{"url":"appendix/synapses.html","title":"突触模型","keywords":"","body":"附录：突触模型 突触动力学模型 AMPA模型 import brainpy as bp class AMPA(bp.TwoEndConn): target_backend = ['numpy', 'numba'] @staticmethod def derivative(s, t, TT, alpha, beta): ds = alpha * TT * (1 - s) - beta * s return ds def __init__(self, pre, post, conn, alpha=0.98, beta=0.18, T=0.5, T_duration=0.5, **kwargs): # parameters self.alpha = alpha self.beta = beta self.T = T self.T_duration = T_duration # connections self.conn = conn(pre.size, post.size) self.pre_ids, self.post_ids = conn.requires('pre_ids', 'post_ids') self.size = len(self.pre_ids) # variables self.s = bp.ops.zeros(self.size) self.t_last_pre_spike = -1e7 * bp.ops.ones(self.size) self.int_s = bp.odeint(f=self.derivative, method='exponential_euler') super(AMPA, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): for i in range(self.size): pre_id = self.pre_ids[i] post_id = self.post_ids[i] if self.pre.spike[pre_id]: self.t_last_pre_spike[pre_id] = _t TT = ((_t - self.t_last_pre_spike[pre_id]) import brainmodels as bm bp.backend.set(backend='numba', dt=0.1) bm.set_backend(backend='numba') def run_syn(syn_model, **kwargs): neu1 = bm.neurons.LIF(2, monitors=['V']) neu2 = bm.neurons.LIF(3, monitors=['V']) syn = syn_model(pre=neu1, post=neu2, conn=bp.connect.All2All(), monitors=['s'], **kwargs) net = bp.Network(neu1, syn, neu2) net.run(30., inputs=(neu1, 'input', 35.)) bp.visualize.line_plot(net.ts, syn.mon.s, ylabel='s', show=True) run_syn(AMPA, T_duration=3.) NMDA模型 class NMDA(bp.TwoEndConn): target_backend = ['numpy', 'numba'] @staticmethod def derivative(s, x, t, tau_rise, tau_decay, a): dsdt = -s / tau_decay + a * x * (1 - s) dxdt = -x / tau_rise return dsdt, dxdt def __init__(self, pre, post, conn, delay=0., g_max=0.15, E=0., cc_Mg=1.2, alpha=0.062, beta=3.57, tau=100, a=0.5, tau_rise=2., **kwargs): # parameters self.g_max = g_max self.E = E self.alpha = alpha self.beta = beta self.cc_Mg = cc_Mg self.tau = tau self.tau_rise = tau_rise self.a = a self.delay = delay # connections self.conn = conn(pre.size, post.size) self.pre_ids, self.post_ids = conn.requires('pre_ids', 'post_ids') self.size = len(self.pre_ids) # variables self.s = bp.ops.zeros(self.size) self.x = bp.ops.zeros(self.size) self.g = self.register_constant_delay('g', size=self.size, delay_time=delay) self.integral = bp.odeint(f=self.derivative, method='rk4') super(NMDA, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): for i in range(self.size): pre_id = self.pre_ids[i] post_id = self.post_ids[i] self.x[i] += self.pre.spike[pre_id] self.s[i], self.x[i] = self.integral(self.s[i], self.x[i], _t, self.tau_rise, self.tau, self.a) # output g_inf_exp = bp.ops.exp(-self.alpha * self.post.V[post_id]) g_inf = 1 + g_inf_exp * self.cc_Mg / self.beta self.g.push(i, self.g_max * self.s[i] / g_inf) I_syn = self.g.pull(i) * (self.post.V[post_id] - self.E) self.post.input[post_id] -= I_syn run_syn(NMDA) GABAB模型 class GABAb(bp.TwoEndConn): target_backend = ['numpy', 'numba'] @staticmethod def derivative(R, G, t, k3, TT, k4, k1, k2): dRdt = k3 * TT * (1 - R) - k4 * R dGdt = k1 * R - k2 * G return dRdt, dGdt def __init__(self, pre, post, conn, delay=0., g_max=0.02, E=-95., k1=0.18, k2=0.034, k3=0.09, k4=0.0012, kd=100., T=0.5, T_duration=0.3, **kwargs): # params self.g_max = g_max self.E = E self.k1 = k1 self.k2 = k2 self.k3 = k3 self.k4 = k4 self.kd = kd self.T = T self.T_duration = T_duration # conns self.conn = conn(pre.size, post.size) self.pre_ids, self.post_ids = conn.requires('pre_ids', 'post_ids') self.size = len(self.pre_ids) # data self.R = bp.ops.zeros(self.size) self.G = bp.ops.zeros(self.size) self.t_last_pre_spike = bp.ops.ones(self.size) * -1e7 self.s = bp.ops.zeros(self.size) self.g = self.register_constant_delay('g', size=self.size, delay_time=delay) self.integral = bp.odeint(f=self.derivative, method='rk4') super(GABAb, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): for i in range(self.size): pre_id = self.pre_ids[i] post_id = self.post_ids[i] if self.pre.spike[pre_id]: self.t_last_pre_spike[i] = _t TT = ((_t - self.t_last_pre_spike[i]) neu1 = bm.neurons.LIF(2, monitors=['V']) neu2 = bm.neurons.LIF(3, monitors=['V']) syn = GABAb(pre=neu1, post=neu2, conn=bp.connect.All2All(), monitors=['s']) net = bp.Network(neu1, syn, neu2) # input I, dur = bp.inputs.constant_current([(25, 20), (0, 1000)]) net.run(dur, inputs=(neu1, 'input', I)) bp.visualize.line_plot(net.ts, syn.mon.s, ylabel='s', show=True) 双指数差（Differences of two exponentials） class Two_exponentials(bp.TwoEndConn): target_backend = ['numpy', 'numba'] @staticmethod def derivative(s, x, t, tau1, tau2): dxdt = (-(tau1 + tau2) * x - s) / (tau1 * tau2) dsdt = x return dsdt, dxdt def __init__(self, pre, post, conn, tau1=1.0, tau2=3.0, **kwargs): # parameters self.tau1 = tau1 self.tau2 = tau2 # connections self.conn = conn(pre.size, post.size) self.pre_ids, self.post_ids = conn.requires('pre_ids', 'post_ids') self.size = len(self.pre_ids) # variables self.s = bp.ops.zeros(self.size) self.x = bp.ops.zeros(self.size) self.integral = bp.odeint(f=self.derivative, method='rk4') super(Two_exponentials, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): for i in range(self.size): pre_id = self.pre_ids[i] self.s[i], self.x[i] = self.integral(self.s[i], self.x[i], _t, self.tau1, self.tau2) self.x[i] += self.pre.spike[pre_id] run_syn(Two_exponentials, tau1=2.) Alpha突触 class Alpha(bp.TwoEndConn): target_backend = ['numpy', 'numba'] @staticmethod def derivative(s, x, t, tau): dxdt = (-2 * tau * x - s) / (tau ** 2) dsdt = x return dsdt, dxdt def __init__(self, pre, post, conn, tau=3.0, **kwargs): # parameters self.tau = tau # connections self.conn = conn(pre.size, post.size) self.pre_ids, self.post_ids = conn.requires('pre_ids', 'post_ids') self.size = len(self.pre_ids) # variables self.s = bp.ops.zeros(self.size) self.x = bp.ops.zeros(self.size) self.integral = bp.odeint(f=self.derivative, method='rk4') super(Alpha, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): for i in range(self.size): pre_id = self.pre_ids[i] self.s[i], self.x[i] = self.integral(self.s[i], self.x[i], _t, self.tau) self.x[i] += self.pre.spike[pre_id] run_syn(Alpha) 单指数衰减（Single exponential decay） class Exponential(bp.TwoEndConn): target_backend = ['numpy', 'numba'] @staticmethod def derivative(s, t, tau): ds = -s / tau return ds def __init__(self, pre, post, conn, tau=8.0, **kwargs): # parameters self.tau = tau # connections self.conn = conn(pre.size, post.size) self.pre_ids, self.post_ids = conn.requires('pre_ids', 'post_ids') self.size = len(self.pre_ids) # variables self.s = bp.ops.zeros(self.size) self.integral = bp.odeint(f=self.derivative, method='exponential_euler') super(Exponential, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): for i in range(self.size): pre_id = self.pre_ids[i] self.s[i] = self.integral(self.s[i], _t, self.tau) self.s[i] += self.pre.spike[pre_id] run_syn(Exponential) 电压跳变（Voltage jump） class Voltage_jump(bp.TwoEndConn): target_backend = ['numpy', 'numba'] def __init__(self, pre, post, conn, **kwargs): # connections self.conn = conn(pre.size, post.size) self.pre_ids, self.post_ids = conn.requires('pre_ids', 'post_ids') self.size = len(self.pre_ids) # variables self.s = bp.ops.zeros(self.size) super(Voltage_jump, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): for i in range(self.size): pre_id = self.pre_ids[i] self.s[i] = self.pre.spike[pre_id] run_syn(Voltage_jump) 缝隙连接（Gap junction） class Gap_junction(bp.TwoEndConn): target_backend = ['numpy', 'numba'] def __init__(self, pre, post, conn, delay=0., k_spikelet=0.1, post_refractory=False, **kwargs): self.delay = delay self.k_spikelet = k_spikelet self.post_has_refractory = post_refractory # connections self.conn = conn(pre.size, post.size) self.pre_ids, self.post_ids = conn.requires('pre_ids', 'post_ids') self.size = len(self.pre_ids) # variables self.w = bp.ops.ones(self.size) self.spikelet = self.register_constant_delay('spikelet', size=self.size, delay_time=self.delay) super(Gap_junction, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): for i in range(self.size): pre_id = self.pre_ids[i] post_id = self.post_ids[i] self.post.input[post_id] += self.w[i] * (self.pre.V[pre_id] - self.post.V[post_id]) self.spikelet.push(i, self.w[i] * self.k_spikelet * self.pre.spike[pre_id]) out = self.spikelet.pull(i) if self.post_has_refractory: self.post.V[post_id] += out * (1. - self.post.refractory[post_id]) else: self.post.V[post_id] += out import matplotlib.pyplot as plt neu0 = bm.neurons.LIF(1, monitors=['V'], t_refractory=0) neu0.V = bp.ops.ones(neu0.V.shape) * -10. neu1 = bm.neurons.LIF(1, monitors=['V'], t_refractory=0) neu1.V = bp.ops.ones(neu1.V.shape) * -10. syn = Gap_junction(pre=neu0, post=neu1, conn=bp.connect.All2All(), k_spikelet=5.) syn.w = bp.ops.ones(syn.w.shape) * .5 net = bp.Network(neu0, neu1, syn) net.run(100., inputs=(neu0, 'input', 30.)) fig, gs = bp.visualize.get_figure(row_num=2, col_num=1, ) fig.add_subplot(gs[1, 0]) plt.plot(net.ts, neu0.mon.V[:, 0], label='V0') plt.legend() fig.add_subplot(gs[0, 0]) plt.plot(net.ts, neu1.mon.V[:, 0], label='V1') plt.legend() plt.show() 突触可塑性模型 突触短时程可塑性（STP） class STP(bp.TwoEndConn): target_backend = ['numpy', 'numba'] @staticmethod def derivative(s, u, x, t, tau, tau_d, tau_f): dsdt = -s / tau dudt = - u / tau_f dxdt = (1 - x) / tau_d return dsdt, dudt, dxdt def __init__(self, pre, post, conn, delay=0., U=0.15, tau_f=1500., tau_d=200., tau=8., **kwargs): # parameters self.tau_d = tau_d self.tau_f = tau_f self.tau = tau self.U = U self.delay = delay # connections self.conn = conn(pre.size, post.size) self.pre_ids, self.post_ids = conn.requires('pre_ids', 'post_ids') self.size = len(self.pre_ids) # variables self.s = bp.ops.zeros(self.size) self.x = bp.ops.ones(self.size) self.u = bp.ops.zeros(self.size) self.w = bp.ops.ones(self.size) self.I_syn = self.register_constant_delay('I_syn', size=self.size, delay_time=delay) self.integral = bp.odeint(f=self.derivative, method='exponential_euler') super(STP, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): for i in range(self.size): pre_id = self.pre_ids[i] self.s[i], u, x = self.integral(self.s[i], self.u[i], self.x[i], _t, self.tau, self.tau_d, self.tau_f) if self.pre.spike[pre_id] > 0: u += self.U * (1 - self.u[i]) self.s[i] += self.w[i] * u * self.x[i] x -= u * self.x[i] self.u[i] = u self.x[i] = x # output post_id = self.post_ids[i] self.I_syn.push(i, self.s[i]) self.post.input[post_id] += self.I_syn.pull(i) def run_stp(**kwargs): neu1 = bm.neurons.LIF(1, monitors=['V']) neu2 = bm.neurons.LIF(1, monitors=['V']) syn = STP(pre=neu1, post=neu2, conn=bp.connect.All2All(), monitors=['s', 'u', 'x'], **kwargs) net = bp.Network(neu1, syn, neu2) net.run(100., inputs=(neu1, 'input', 28.)) # plot fig, gs = bp.visualize.get_figure(2, 1, 3, 7) fig.add_subplot(gs[0, 0]) plt.plot(net.ts, syn.mon.u[:, 0], label='u') plt.plot(net.ts, syn.mon.x[:, 0], label='x') plt.legend() fig.add_subplot(gs[1, 0]) plt.plot(net.ts, syn.mon.s[:, 0], label='s') plt.legend() plt.xlabel('Time (ms)') plt.show() run_stp(U=0.2, tau_d=150., tau_f=2.) run_stp(U=0.1, tau_d=10, tau_f=100.) 脉冲时间依赖可塑性（STDP） class STDP(bp.TwoEndConn): target_backend = ['numpy', 'numba'] @staticmethod def derivative(s, A_s, A_t, t, tau, tau_s, tau_t): dsdt = -s / tau dAsdt = - A_s / tau_s dAtdt = - A_t / tau_t return dsdt, dAsdt, dAtdt def __init__(self, pre, post, conn, delay=0., delta_A_s=0.5, delta_A_t=0.5, w_min=0., w_max=20., tau_s=10., tau_t=10., tau=10., **kwargs): # parameters self.tau_s = tau_s self.tau_t = tau_t self.tau = tau self.delta_A_s = delta_A_s self.delta_A_t = delta_A_t self.w_min = w_min self.w_max = w_max self.delay = delay # connections self.conn = conn(pre.size, post.size) self.pre_ids, self.post_ids = self.conn.requires('pre_ids', 'post_ids') self.size = len(self.pre_ids) # variables self.s = bp.ops.zeros(self.size) self.A_s = bp.ops.zeros(self.size) self.A_t = bp.ops.zeros(self.size) self.w = bp.ops.ones(self.size) * 1. self.I_syn = self.register_constant_delay('I_syn', size=self.size, delay_time=delay) self.integral = bp.odeint(f=self.derivative, method='exponential_euler') super(STDP, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): for i in range(self.size): pre_id = self.pre_ids[i] post_id = self.post_ids[i] self.s[i], A_s, A_t = self.integral(self.s[i], self.A_s[i], self.A_t[i], _t, self.tau, self.tau_s, self.tau_t) w = self.w[i] if self.pre.spike[pre_id] > 0: self.s[i] += w A_s += self.delta_A_s w -= A_t if self.post.spike[post_id] > 0: A_t += self.delta_A_t w += A_s self.A_s[i] = A_s self.A_t[i] = A_t self.w[i] = bp.ops.clip(w, self.w_min, self.w_max) # output self.I_syn.push(i, self.s[i]) self.post.input[post_id] += self.I_syn.pull(i) duration = 300. (I_pre, _) = bp.inputs.constant_current([(0, 5), (30, 15), # pre at 5ms (0, 15), (30, 15), (0, 15), (30, 15), (0, 98), (30, 15), # switch order: t_interval=98ms (0, 15), (30, 15), (0, 15), (30, 15), (0, duration-155-98)]) (I_post, _) = bp.inputs.constant_current([(0, 10), (30, 15), # post at 10 (0, 15), (30, 15), (0, 15), (30, 15), (0, 90), (30, 15), # switch order: t_interval=98-8=90(ms) (0, 15), (30, 15), (0, 15), (30, 15), (0, duration-160-90)]) pre = bm.neurons.LIF(1, monitors=['spike']) post = bm.neurons.LIF(1, monitors=['spike']) syn = STDP(pre=pre, post=post, conn=bp.connect.All2All(), monitors=['s', 'w']) net = bp.Network(pre, syn, post) net.run(duration, inputs=[(pre, 'input', I_pre), (post, 'input', I_post)]) # plot fig, gs = bp.visualize.get_figure(4, 1, 2, 7) def hide_spines(my_ax): plt.legend() plt.xticks([]) plt.yticks([]) my_ax.spines['left'].set_visible(False) my_ax.spines['right'].set_visible(False) my_ax.spines['bottom'].set_visible(False) my_ax.spines['top'].set_visible(False) ax=fig.add_subplot(gs[0, 0]) plt.plot(net.ts, syn.mon.s[:, 0], label=\"s\") hide_spines(ax) ax1=fig.add_subplot(gs[1, 0]) plt.plot(net.ts, pre.mon.spike[:, 0], label=\"pre spike\") plt.ylim(0, 2) hide_spines(ax1) plt.legend(loc = 'center right') ax2=fig.add_subplot(gs[2, 0]) plt.plot(net.ts, post.mon.spike[:, 0], label=\"post spike\") plt.ylim(-1, 1) hide_spines(ax2) ax3=fig.add_subplot(gs[3, 0]) plt.plot(net.ts, syn.mon.w[:, 0], label=\"w\") plt.legend() # hide spines plt.yticks([]) ax3.spines['left'].set_visible(False) ax3.spines['right'].set_visible(False) ax3.spines['top'].set_visible(False) plt.xlabel('Time (ms)') plt.show() Oja法则 import numpy as np bp.backend.set(backend='numpy', dt=0.1) class Oja(bp.TwoEndConn): target_backend = 'numpy' @staticmethod def derivative(w, t, gamma, r_pre, r_post): dwdt = gamma * (r_post * r_pre - r_post * r_post * w) return dwdt def __init__(self, pre, post, conn, gamma=.005, w_max=1., w_min=0., **kwargs): # params self.gamma = gamma self.w_max = w_max self.w_min = w_min # no delay in firing rate models # conns self.conn = conn(pre.size, post.size) self.conn_mat = conn.requires('conn_mat') self.size = bp.ops.shape(self.conn_mat) # data self.w = bp.ops.ones(self.size) * 0.05 self.integral = bp.odeint(f=self.derivative) super(Oja, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): w = self.conn_mat * self.w self.post.r = np.sum(w.T * self.pre.r, axis=1) # resize to matrix dim = self.size r_post = np.vstack((self.post.r,) * dim[0]) r_pre = np.vstack((self.pre.r,) * dim[1]).T self.w = self.integral(w, _t, self.gamma, r_pre, r_post) class neu(bp.NeuGroup): target_backend = 'numpy' def __init__(self, size, **kwargs): self.r = bp.ops.zeros(size) super(neu, self).__init__(size=size, **kwargs) def update(self, _t): self.r = self.r # create input current1, _ = bp.inputs.constant_current([(2., 20.), (0., 20.)] * 3 + [(0., 20.), (0., 20.)] * 2) current2, _ = bp.inputs.constant_current([(2., 20.), (0., 20.)] * 5) current3, _ = bp.inputs.constant_current([(2., 20.), (0., 20.)] * 5) current_pre = np.vstack((current1, current2)) current_post = np.vstack((current3, current3)) # simulate neu_pre = neu(2, monitors=['r']) neu_post = neu(2, monitors=['r']) syn = Oja(pre=neu_pre, post=neu_post, conn=bp.connect.All2All(), monitors=['w']) net = bp.Network(neu_pre, syn, neu_post) net.run(duration=200., inputs=[(neu_pre, 'r', current_pre.T, '='), (neu_post, 'r', current_post.T)]) # plot fig, gs = bp.visualize.get_figure(4, 1, 2, 6) fig.add_subplot(gs[0, 0]) plt.plot(net.ts, neu_pre.mon.r[:, 0], 'b', label='pre r1') plt.legend() fig.add_subplot(gs[1, 0]) plt.plot(net.ts, neu_pre.mon.r[:, 1], 'r', label='pre r2') plt.legend() fig.add_subplot(gs[2, 0]) plt.plot(net.ts, neu_post.mon.r[:, 0], color='purple', label='post r') plt.ylim([0, 4]) plt.legend() fig.add_subplot(gs[3, 0]) plt.plot(net.ts, syn.mon.w[:, 0, 0], 'b', label='syn.w1') plt.plot(net.ts, syn.mon.w[:, 1, 0], 'r', label='syn.w2') plt.legend() plt.show() BCM法则 class BCM(bp.TwoEndConn): target_backend = ['numpy', 'numba'] @staticmethod def derivative(w, t, lr, r_pre, r_post, r_th): dwdt = lr * r_post * (r_post - r_th) * r_pre return dwdt def __init__(self, pre, post, conn, lr=0.005, w_max=1., w_min=0., **kwargs): # parameters self.lr = lr self.w_max = w_max self.w_min = w_min self.dt = bp.backend.get_dt() # connections self.conn = conn(pre.size, post.size) self.conn_mat = conn.requires('conn_mat') self.size = bp.ops.shape(self.conn_mat) # variables self.w = bp.ops.ones(self.size) * .5 self.sum_post_r = bp.ops.zeros(post.size[0]) self.r_th = bp.ops.zeros(post.size[0]) self.int_w = bp.odeint(f=self.derivative, method='rk4') super(BCM, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): # update threshold self.sum_post_r += self.post.r r_th = self.sum_post_r / (_t / self.dt + 1) self.r_th = r_th # resize to matrix w = self.w * self.conn_mat dim = self.size r_th = np.vstack((r_th,) * dim[0]) r_post = np.vstack((self.post.r,) * dim[0]) r_pre = np.vstack((self.pre.r,) * dim[1]).T # update w w = self.int_w(w, _t, self.lr, r_pre, r_post, r_th) self.w = np.clip(w, self.w_min, self.w_max) # output self.post.r = np.sum(w.T * self.pre.r, axis=1) # create input group1, _ = bp.inputs.constant_current(([1.5, 1], [0, 1]) * 10) group2, duration = bp.inputs.constant_current(([0, 1], [1., 1]) * 10) group1 = np.vstack(((group1,) * 10)) group2 = np.vstack(((group2,) * 10)) input_r = np.vstack((group1, group2)) # simulate pre = neu(20, monitors=['r']) post = neu(1, monitors=['r']) bcm = BCM(pre=pre, post=post, conn=bp.connect.All2All(), monitors=['w']) net = bp.Network(pre, bcm, post) net.run(duration, inputs=(pre, 'r', input_r.T, \"=\")) # plot fig, gs = bp.visualize.get_figure(2, 1) fig.add_subplot(gs[1, 0], xlim=(0, duration), ylim=(0, bcm.w_max)) plt.plot(net.ts, bcm.mon.w[:, 0], 'b', label='w1') plt.plot(net.ts, bcm.mon.w[:, 11], 'r', label='w2') plt.title(\"weights\") plt.ylabel(\"weights\") plt.xlabel(\"t\") plt.legend() fig.add_subplot(gs[0, 0], xlim=(0, duration)) plt.plot(net.ts, pre.mon.r[:, 0], 'b', label='r1') plt.plot(net.ts, pre.mon.r[:, 11], 'r', label='r2') plt.title(\"inputs\") plt.ylabel(\"firing rate\") plt.xlabel(\"t\") plt.legend() plt.show() "},"appendix/networks.html":{"url":"appendix/networks.html","title":"网络模型","keywords":"","body":"脉冲神经网络 兴奋-抑制平衡网络 # -*- coding: utf-8 -*- import brainpy as bp import brainmodels import matplotlib.pyplot as plt import numpy as np bp.backend.set('numba') N_E = 500 N_I = 500 prob = 0.1 tau = 10. V_rest = -52. V_reset = -60. V_th = -50. tau_decay = 2. neu_E = brainmodels.neurons.LIF(N_E, monitors=['spike']) neu_I = brainmodels.neurons.LIF(N_I, monitors=['spike']) neu_E.V = V_rest + np.random.random(N_E) * (V_th - V_rest) neu_I.V = V_rest + np.random.random(N_I) * (V_th - V_rest) syn_E2E = brainmodels.synapses.Exponential(pre=neu_E, post=neu_E, conn=bp.connect.FixedProb(prob=prob)) syn_E2I = brainmodels.synapses.Exponential(pre=neu_E, post=neu_I, conn=bp.connect.FixedProb(prob=prob)) syn_I2E = brainmodels.synapses.Exponential(pre=neu_I, post=neu_E, conn=bp.connect.FixedProb(prob=prob)) syn_I2I = brainmodels.synapses.Exponential(pre=neu_I, post=neu_I, conn=bp.connect.FixedProb(prob=prob)) JE = 1 / np.sqrt(prob * N_E) JI = 1 / np.sqrt(prob * N_I) syn_E2E.w = JE syn_E2I.w = JE syn_I2E.w = -JI syn_I2I.w = -JI net = bp.Network(neu_E, neu_I, syn_E2E, syn_E2I, syn_I2E, syn_I2I) net.run(500., inputs=[(neu_E, 'input', 3.), (neu_I, 'input', 3.)], report=True) fig, gs = bp.visualize.get_figure(4, 1, 2, 10) fig.add_subplot(gs[:3, 0]) bp.visualization.raster_plot(net.ts, neu_E.mon.spike) fig.add_subplot(gs[3, 0]) rate = bp.measure.firing_rate(neu_E.mon.spike, 5.) plt.plot(net.ts, rate) plt.show() 抉择网络 # -*- coding: utf-8 -*- \"\"\" Implementation of the paper: Wang, Xiao-Jing. \"Probabilistic decision making by slow reverberation in cortical circuits.\" Neuron 36.5 (2002): 955-968. \"\"\" import brainpy as bp import numpy as np import matplotlib.pyplot as plt # set params # set global params dt = 0.05 # ms method = 'exponential' bp.backend.set('numpy', dt=dt) # set network params base_N_E = 1600 base_N_I = 400 net_scale = 5. N_E = int(base_N_E // net_scale) N_I = int(base_N_I // net_scale) f = 0.15 # Note: proportion of neurons activated by one of the two stimulus N_A = int(f * N_E) N_B = int(f * N_E) N_non = N_E - N_A - N_B # Note: N_E = N_A + N_B + N_non print(f\"N_E = {N_E} = {N_A} + {N_B} + {N_non}, N_I = {N_I}\") # Note: N_E[0:N_A]: A_group # N_E[N_A : N_A+N_B]: B_group # N_E[N_A + N_B: N_E]: non of A or B time_scale = 1. pre_period = 100. / time_scale stim_period = 1000. delay_period = 500. / time_scale total_period = pre_period + stim_period + delay_period # set LIF neu params V_rest_E = -70. # mV V_reset_E = -55. # mV V_th_E = -50. # mV g_E = 25. * 1e-3 # uS R_E = 1 / g_E # MOhm C_E = 0.5 # nF tau_E = 20. # ms t_refractory_E = 2. # ms print(f\"R_E * C_E = {R_E * C_E} should be equal to tau_E = {tau_E}\") V_rest_I = -70. # mV V_reset_I = -55. # mV V_th_I = -50. # mV g_I = 20. * 1e-3 # uS R_I = 1 / g_I # Mohm C_I = 0.2 # nF tau_I = 10. # ms t_refractory_I = 1. # ms print(f\"R_I * C_I = {R_I * C_I} should be equal to tau_I = {tau_I}\") class LIF(bp.NeuGroup): target_backend = 'general' @staticmethod def derivative(V, t, I_ext, V_rest, R, tau): dvdt = (- (V - V_rest) + R * I_ext) / tau return dvdt def __init__(self, size, V_rest=0., V_reset=0., V_th=0., R=0., tau=0., t_refractory=0., **kwargs): self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.R = R self.tau = tau self.t_refractory = t_refractory self.V = bp.ops.zeros(size) self.input = bp.ops.zeros(size) self.spike = bp.ops.zeros(size, dtype=bool) self.refractory = bp.ops.zeros(size, dtype=bool) self.t_last_spike = bp.ops.ones(size) * -1e7 self.integral = bp.odeint(self.derivative) super(LIF, self).__init__(size=size, **kwargs) def update(self, _t): # update variables not_ref = (_t - self.t_last_spike > self.t_refractory) self.V[not_ref] = self.integral( self.V[not_ref], _t, self.input[not_ref], self.V_rest, self.R, self.tau) sp = (self.V > self.V_th) self.V[sp] = self.V_reset self.t_last_spike[sp] = _t self.spike = sp self.refractory = ~not_ref self.input[:] = 0. # set syn params E_AMPA = 0. # mV tau_decay_AMPA = 2 # ms E_NMDA = 0. # mV alpha_NMDA = 0.062 # \\ beta_NMDA = 3.57 # \\ cc_Mg_NMDA = 1. # mM a_NMDA = 0.5 # kHz/ms^-1 tau_rise_NMDA = 2. # ms tau_decay_NMDA = 100. # ms E_GABAa = -70. # mV tau_decay_GABAa = 5. # ms delay_syn = 0.5 # ms class NMDA(bp.TwoEndConn): target_backend = 'general' @staticmethod def derivative(s, x, t, tau_rise, tau_decay, a): dxdt = -x / tau_rise dsdt = -s / tau_decay + a * x * (1 - s) return dsdt, dxdt def __init__(self, pre, post, conn, delay=0., g_max=0.15, E=0., cc_Mg=1.2, alpha=0.062, beta=3.57, tau=100, a=0.5, tau_rise=2., **kwargs): # parameters self.g_max = g_max self.E = E self.alpha = alpha self.beta = beta self.cc_Mg = cc_Mg self.tau = tau self.tau_rise = tau_rise self.a = a self.delay = delay # connections self.conn = conn(pre.size, post.size) self.conn_mat = conn.requires('conn_mat') self.size = bp.ops.shape(self.conn_mat) # variables self.s = bp.ops.zeros(self.size) self.x = bp.ops.zeros(self.size) self.g = self.register_constant_delay('g', size=self.size, delay_time=delay) self.integral = bp.odeint(self.derivative) super(NMDA, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): self.x += bp.ops.unsqueeze(self.pre.spike, 1) * self.conn_mat self.s, self.x = self.integral(self.s, self.x, _t, self.tau_rise, self.tau, self.a) self.g.push(self.g_max * self.s) g_inf = 1 + self.cc_Mg / self.beta * \\ bp.ops.exp(-self.alpha * self.post.V) g_inf = 1 / g_inf self.post.input -= bp.ops.sum(self.g.pull(), axis=0) * \\ (self.post.V - self.E) * g_inf class AMPA(bp.TwoEndConn): target_backend = 'general' @staticmethod def derivative(s, t, tau): ds = - s / tau return ds def __init__(self, pre, post, conn, delay=0., g_max=0.10, E=0., tau=2.0, **kwargs): # parameters self.g_max = g_max self.E = E self.tau = tau self.delay = delay # connections self.conn = conn(pre.size, post.size) self.conn_mat = conn.requires('conn_mat') self.size = bp.ops.shape(self.conn_mat) # data self.s = bp.ops.zeros(self.size) self.g = self.register_constant_delay('g', size=self.size, delay_time=delay) self.int_s = bp.odeint(f=self.derivative, method='euler') super(AMPA, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): self.s = self.int_s(self.s, _t, self.tau) self.s += bp.ops.unsqueeze(self.pre.spike, 1) * self.conn_mat self.g.push(self.g_max * self.s) self.post.input -= bp.ops.sum(self.g.pull(), 0) * (self.post.V - self.E) class GABAa(bp.TwoEndConn): target_backend = 'general' @staticmethod def derivative(s, t, tau_decay): dsdt = - s / tau_decay return dsdt def __init__(self, pre, post, conn, delay=0., g_max=0.4, E=-80., tau_decay=6., **kwargs): # parameters self.g_max = g_max self.E = E self.tau_decay = tau_decay self.delay = delay # connections self.conn = conn(pre.size, post.size) self.conn_mat = conn.requires('conn_mat') self.size = bp.ops.shape(self.conn_mat) # data self.s = bp.ops.zeros(self.size) self.g = self.register_constant_delay('g', size=self.size, delay_time=delay) self.integral = bp.odeint(self.derivative) super(GABAa, self).__init__(pre=pre, post=post, **kwargs) def update(self, _t): self.s = self.integral(self.s, _t, self.tau_decay) for i in range(self.pre.size[0]): if self.pre.spike[i] > 0: self.s[i] += self.conn_mat[i] self.g.push(self.g_max * self.s) g = self.g.pull() self.post.input -= bp.ops.sum(g, axis=0) * (self.post.V - self.E) # set syn weights (only used in recurrent E connections) w_pos = 1.7 w_neg = 1. - f * (w_pos - 1.) / (1. - f) print(f\"the structured weight is: w_pos = {w_pos}, w_neg = {w_neg}\") # inside select group: w = w+ # between group / from non-select group to select group: w = w- # A2A B2B w+, A2B B2A w-, non2A non2B w- weight = np.ones((N_E, N_E), dtype=np.float) for i in range(N_A): weight[i, 0: N_A] = w_pos weight[i, N_A: N_A + N_B] = w_neg for i in range(N_A, N_A + N_B): weight[i, N_A: N_A + N_B] = w_pos weight[i, 0: N_A] = w_neg for i in range(N_A + N_B, N_E): weight[i, 0: N_A + N_B] = w_neg print(f\"Check constraints: Weight sum {weight.sum(axis=0)[0]} \\ should be equal to N_E = {N_E}\") # set background params poisson_freq = 2400. # Hz g_max_ext2E_AMPA = 2.1 * 1e-3 # uS g_max_ext2I_AMPA = 1.62 * 1e-3 # uS g_max_E2E_AMPA = 0.05 * 1e-3 * net_scale g_max_E2E_NMDA = 0.165 * 1e-3 * net_scale g_max_E2I_AMPA = 0.04 * 1e-3 * net_scale g_max_E2I_NMDA = 0.13 * 1e-3 * net_scale g_max_I2E_GABAa = 1.3 * 1e-3 * net_scale g_max_I2I_GABAa = 1.0 * 1e-3 * net_scale # def neurons # def E neurons/pyramid neurons neu_A = LIF(N_A, monitors=['spike', 'input', 'V']) neu_A.V_rest = V_rest_E neu_A.V_reset = V_reset_E neu_A.V_th = V_th_E neu_A.R = R_E neu_A.tau = tau_E neu_A.t_refractory = t_refractory_E neu_A.V = bp.ops.ones(N_A) * V_rest_E neu_B = LIF(N_B, monitors=['spike', 'input', 'V']) neu_B.V_rest = V_rest_E neu_B.V_reset = V_reset_E neu_B.V_th = V_th_E neu_B.R = R_E neu_B.tau = tau_E neu_B.t_refractory = t_refractory_E neu_B.V = bp.ops.ones(N_B) * V_rest_E neu_non = LIF(N_non, monitors=['spike', 'input', 'V']) neu_non.V_rest = V_rest_E neu_non.V_reset = V_reset_E neu_non.V_th = V_th_E neu_non.R = R_E neu_non.tau = tau_E neu_non.t_refractory = t_refractory_E neu_non.V = bp.ops.ones(N_non) * V_rest_E # def I neurons/interneurons neu_I = LIF(N_I, monitors=['input', 'V']) neu_I.V_rest = V_rest_I neu_I.V_reset = V_reset_I neu_I.V_th = V_th_I neu_I.R = R_I neu_I.tau = tau_I neu_I.t_refractory = t_refractory_I neu_I.V = bp.ops.ones(N_I) * V_rest_I # def synapse connections ## define E2E conn syn_A2A_AMPA = AMPA(pre=neu_A, post=neu_A, conn=bp.connect.All2All(), delay=delay_syn) syn_A2A_NMDA = NMDA(pre=neu_A, post=neu_A, conn=bp.connect.All2All(), delay=delay_syn) syn_A2B_AMPA = AMPA(pre=neu_A, post=neu_B, conn=bp.connect.All2All(), delay=delay_syn) syn_A2B_NMDA = NMDA(pre=neu_A, post=neu_B, conn=bp.connect.All2All(), delay=delay_syn) syn_A2non_AMPA = AMPA(pre=neu_A, post=neu_non, conn=bp.connect.All2All(), delay=delay_syn) syn_A2non_NMDA = NMDA(pre=neu_A, post=neu_non, conn=bp.connect.All2All(), delay=delay_syn) syn_B2A_AMPA = AMPA(pre=neu_B, post=neu_A, conn=bp.connect.All2All(), delay=delay_syn) syn_B2A_NMDA = NMDA(pre=neu_B, post=neu_A, conn=bp.connect.All2All(), delay=delay_syn) syn_B2B_AMPA = AMPA(pre=neu_B, post=neu_B, conn=bp.connect.All2All(), delay=delay_syn) syn_B2B_NMDA = NMDA(pre=neu_B, post=neu_B, conn=bp.connect.All2All(), delay=delay_syn) syn_B2non_AMPA = AMPA(pre=neu_B, post=neu_non, conn=bp.connect.All2All(), delay=delay_syn) syn_B2non_NMDA = NMDA(pre=neu_B, post=neu_non, conn=bp.connect.All2All(), delay=delay_syn) syn_non2A_AMPA = AMPA(pre=neu_non, post=neu_A, conn=bp.connect.All2All(), delay=delay_syn) syn_non2A_NMDA = NMDA(pre=neu_non, post=neu_A, conn=bp.connect.All2All(), delay=delay_syn) syn_non2B_AMPA = AMPA(pre=neu_non, post=neu_B, conn=bp.connect.All2All(), delay=delay_syn) syn_non2B_NMDA = NMDA(pre=neu_non, post=neu_B, conn=bp.connect.All2All(), delay=delay_syn) syn_non2non_AMPA = AMPA(pre=neu_non, post=neu_non, conn=bp.connect.All2All(), delay=delay_syn) syn_non2non_NMDA = NMDA(pre=neu_non, post=neu_non, conn=bp.connect.All2All(), delay=delay_syn) syn_A2A_AMPA.g_max = g_max_E2E_AMPA * w_pos syn_A2A_NMDA.g_max = g_max_E2E_NMDA * w_pos syn_A2B_AMPA.g_max = g_max_E2E_AMPA * w_neg syn_A2B_NMDA.g_max = g_max_E2E_NMDA * w_neg syn_A2non_AMPA.g_max = g_max_E2E_AMPA syn_A2non_NMDA.g_max = g_max_E2E_NMDA syn_B2A_AMPA.g_max = g_max_E2E_AMPA * w_neg syn_B2A_NMDA.g_max = g_max_E2E_NMDA * w_neg syn_B2B_AMPA.g_max = g_max_E2E_AMPA * w_pos syn_B2B_NMDA.g_max = g_max_E2E_NMDA * w_pos syn_B2non_AMPA.g_max = g_max_E2E_AMPA syn_B2non_NMDA.g_max = g_max_E2E_NMDA syn_non2A_AMPA.g_max = g_max_E2E_AMPA * w_neg syn_non2A_NMDA.g_max = g_max_E2E_NMDA * w_neg syn_non2B_AMPA.g_max = g_max_E2E_AMPA * w_neg syn_non2B_NMDA.g_max = g_max_E2E_NMDA * w_neg syn_non2non_AMPA.g_max = g_max_E2E_AMPA syn_non2non_NMDA.g_max = g_max_E2E_NMDA for i in [syn_A2A_AMPA, syn_A2B_AMPA, syn_A2non_AMPA, syn_B2A_AMPA, syn_B2B_AMPA, syn_B2non_AMPA, syn_non2A_AMPA, syn_non2B_AMPA, syn_non2non_AMPA]: i.E = E_AMPA i.tau_decay = tau_decay_AMPA i.E = E_NMDA for i in [syn_A2A_NMDA, syn_A2B_NMDA, syn_A2non_NMDA, syn_B2A_NMDA, syn_B2B_NMDA, syn_B2non_NMDA, syn_non2A_NMDA, syn_non2B_NMDA, syn_non2non_NMDA]: i.alpha = alpha_NMDA i.beta = beta_NMDA i.cc_Mg = cc_Mg_NMDA i.a = a_NMDA i.tau_decay = tau_decay_NMDA i.tau_rise = tau_rise_NMDA ## define E2I conn syn_A2I_AMPA = AMPA(pre=neu_A, post=neu_I, conn=bp.connect.All2All(), delay=delay_syn) syn_A2I_NMDA = NMDA(pre=neu_A, post=neu_I, conn=bp.connect.All2All(), delay=delay_syn) syn_B2I_AMPA = AMPA(pre=neu_B, post=neu_I, conn=bp.connect.All2All(), delay=delay_syn) syn_B2I_NMDA = NMDA(pre=neu_B, post=neu_I, conn=bp.connect.All2All(), delay=delay_syn) syn_non2I_AMPA = AMPA(pre=neu_non, post=neu_I, conn=bp.connect.All2All(), delay=delay_syn) syn_non2I_NMDA = NMDA(pre=neu_non, post=neu_I, conn=bp.connect.All2All(), delay=delay_syn) for i in [syn_A2I_AMPA, syn_B2I_AMPA, syn_non2I_AMPA]: i.g_max = g_max_E2I_AMPA i.E = E_AMPA i.tau_decay = tau_decay_AMPA for i in [syn_A2I_NMDA, syn_B2I_NMDA, syn_non2I_NMDA]: i.g_max = g_max_E2I_NMDA i.E = E_NMDA i.alpha = alpha_NMDA i.beta = beta_NMDA i.cc_Mg = cc_Mg_NMDA i.a = a_NMDA i.tau_decay = tau_decay_NMDA i.tau_rise = tau_rise_NMDA ## define I2E conn syn_I2A_GABAa = GABAa(pre=neu_I, post=neu_A, conn=bp.connect.All2All(), delay=delay_syn) syn_I2B_GABAa = GABAa(pre=neu_I, post=neu_B, conn=bp.connect.All2All(), delay=delay_syn) syn_I2non_GABAa = GABAa(pre=neu_I, post=neu_non, conn=bp.connect.All2All(), delay=delay_syn) for i in [syn_I2A_GABAa, syn_I2B_GABAa, syn_I2non_GABAa]: i.g_max = g_max_I2E_GABAa i.E = E_GABAa i.tau_decay = tau_decay_GABAa ## define I2I conn syn_I2I_GABAa = GABAa(pre=neu_I, post=neu_I, conn=bp.connect.All2All(), delay=delay_syn) syn_I2I_GABAa.g_max = g_max_I2I_GABAa syn_I2I_GABAa.E = E_GABAa syn_I2I_GABAa.tau_decay = tau_decay_GABAa # def background poisson input class PoissonInput(bp.NeuGroup): target_backend = 'general' def __init__(self, size, freqs, dt, **kwargs): self.freqs = freqs self.dt = dt self.spike = bp.ops.zeros(size, dtype=bool) super(PoissonInput, self).__init__(size=size, **kwargs) def update(self, _t): self.spike = np.random.random(self.size) to during the simulation, the neuron generates a poisson spike with frequency . however, the value of changes every ms and obey a Gaussian distribution defined by and . \"\"\" target_backend = 'general' def __init__(self, size, dt=0., t_start=0., t_end=0., t_interval=0., mean_freq=0., var_freq=20., **kwargs): self.dt = dt self.stim_start_t = t_start self.stim_end_t = t_end self.stim_change_freq_interval = t_interval self.mean_freq = mean_freq self.var_freq = var_freq self.freq = 0. self.t_last_change_freq = -1e7 self.spike = bp.ops.zeros(size, dtype=bool) super(PoissonStim, self).__init__(size=size, **kwargs) def update(self, _t): if self.stim_start_t 发放率神经网络 抉择模型 from collections import OrderedDict import brainpy as bp bp.backend.set(backend='numba', dt=0.1) class Decision(bp.NeuGroup): target_backend = ['numpy', 'numba'] @staticmethod def derivative(s1, s2, t, I, coh, JAext, J_rec, J_inh, I_0, a, b, d, tau_s, gamma): I1 = JAext * I * (1. + coh) I2 = JAext * I * (1. - coh) I_syn1 = J_rec * s1 - J_inh * s2 + I_0 + I1 r1 = (a * I_syn1 - b) / (1. - bp.ops.exp(-d * (a * I_syn1 - b))) ds1dt = - s1 / tau_s + (1. - s1) * gamma * r1 I_syn2 = J_rec * s2 - J_inh * s1 + I_0 + I2 r2 = (a * I_syn2 - b) / (1. - bp.ops.exp(-d * (a * I_syn2 - b))) ds2dt = - s2 / tau_s + (1. - s2) * gamma * r2 return ds1dt, ds2dt def __init__(self, size, coh, JAext=.00117, J_rec=.3725, J_inh=.1137, I_0=.3297, a=270., b=108., d=0.154, tau_s=.06, gamma=0.641, **kwargs): # parameters self.coh = coh self.JAext = JAext self.J_rec = J_rec self.J_inh = J_inh self.I0 = I_0 self.a = a self.b = b self.d = d self.tau_s = tau_s self.gamma = gamma # variables self.s1 = bp.ops.ones(size) * .06 self.s2 = bp.ops.ones(size) * .06 self.input = bp.ops.zeros(size) self.integral = bp.odeint(f=self.derivative, method='rk4', dt=0.01) super(Decision, self).__init__(size=size, **kwargs) def update(self, _t): for i in range(self.size): self.s1[i], self.s2[i] = self.integral(self.s1[i], self.s2[i], _t, self.input[i], self.coh, self.JAext, self.J_rec, self.J_inh, self.I0, self.a, self.b, self.d, self.tau_s, self.gamma) self.input[i] = 0. def phase_analyze(I, coh): decision = Decision(1, coh=coh) phase = bp.analysis.PhasePlane(decision.integral, target_vars=OrderedDict(s2=[0., 1.], s1=[0., 1.]), fixed_vars=None, pars_update=dict(I=I, coh=coh, JAext=.00117, J_rec=.3725, J_inh=.1137, I_0=.3297, a=270., b=108., d=0.154, tau_s=.06, gamma=0.641), numerical_resolution=.001, options={'escape_sympy_solver': True}) phase.plot_nullcline() phase.plot_fixed_point() phase.plot_vector_field(show=True) # no input phase_analyze(I=0., coh=0.) # coherence = 0% print(\"coherence = 0%\") phase_analyze(I=30., coh=0.) # coherence = 51.2% print(\"coherence = 51.2%\") phase_analyze(I=30., coh=0.512) # coherence = 100% print(\"coherence = 100%\") phase_analyze(I=30., coh=1.) 连续吸引子模型（CANN） import brainpy as bp import numpy as np bp.backend.set(backend='numpy', dt=0.1) class CANN1D(bp.NeuGroup): target_backend = ['numpy', 'numba'] def __init__(self, num, tau=1., k=8.1, a=0.5, A=10., J0=4., z_min=-np.pi, z_max=np.pi, **kwargs): # parameters self.tau = tau # The synaptic time constant self.k = k # Degree of the rescaled inhibition self.a = a # Half-width of the range of excitatory connections self.A = A # Magnitude of the external input self.J0 = J0 # maximum connection value # feature space self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = np.linspace(z_min, z_max, num) # The encoded feature values # variables self.u = np.zeros(num) self.input = np.zeros(num) # The connection matrix self.conn_mat = self.make_conn(self.x) super(CANN1D, self).__init__(size=num, **kwargs) self.rho = num / self.z_range # The neural density self.dx = self.z_range / num # The stimulus density @staticmethod @bp.odeint(method='rk4', dt=0.05) def int_u(u, t, conn, k, tau, Iext): r1 = np.square(u) r2 = 1.0 + k * np.sum(r1) r = r1 / r2 Irec = np.dot(conn, r) du = (-u + Irec + Iext) / tau return du def dist(self, d): d = np.remainder(d, self.z_range) d = np.where(d > 0.5 * self.z_range, d - self.z_range, d) return d def make_conn(self, x): assert np.ndim(x) == 1 x_left = np.reshape(x, (-1, 1)) x_right = np.repeat(x.reshape((1, -1)), len(x), axis=0) d = self.dist(x_left - x_right) Jxx = self.J0 * np.exp(-0.5 * np.square(d / self.a)) / ( np.sqrt(2 * np.pi) * self.a) return Jxx def get_stimulus_by_pos(self, pos): return self.A * np.exp(-0.25 * np.square(self.dist(self.x - pos) / self.a)) def update(self, _t): self.u = self.int_u(self.u, _t, self.conn_mat, self.k, self.tau, self.input) self.input[:] = 0. def plot_animate(frame_step=5, frame_delay=50): bp.visualize.animate_1D(dynamical_vars=[{'ys': cann.mon.u, 'xs': cann.x, 'legend': 'u'}, {'ys': Iext, 'xs': cann.x, 'legend': 'Iext'}], frame_step=frame_step, frame_delay=frame_delay, show=True) cann = CANN1D(num=512, k=0.1, monitors=['u']) I1 = cann.get_stimulus_by_pos(0.) Iext, duration = bp.inputs.constant_current([(0., 1.), (I1, 8.), (0., 8.)]) cann.run(duration=duration, inputs=('input', Iext)) # define function def plot_animate(frame_step=5, frame_delay=50): bp.visualize.animate_1D(dynamical_vars=[{'ys': cann.mon.u, 'xs': cann.x, 'legend': 'u'}, {'ys': Iext, 'xs': cann.x, 'legend': 'Iext'}], frame_step=frame_step, frame_delay=frame_delay, show=True) # call the function plot_animate(frame_step=1, frame_delay=100) cann = CANN1D(num=512, k=8.1, monitors=['u']) dur1, dur2, dur3 = 10., 30., 0. num1 = int(dur1 / bp.backend.get_dt()) num2 = int(dur2 / bp.backend.get_dt()) num3 = int(dur3 / bp.backend.get_dt()) Iext = np.zeros((num1 + num2 + num3,) + cann.size) Iext[:num1] = cann.get_stimulus_by_pos(0.5) Iext[num1:num1 + num2] = cann.get_stimulus_by_pos(0.) Iext[num1:num1 + num2] += 0.1 * cann.A * np.random.randn(num2, *cann.size) cann.run(duration=dur1 + dur2 + dur3, inputs=('input', Iext)) plot_animate() cann = CANN1D(num=512, k=8.1, monitors=['u']) dur1, dur2, dur3 = 20., 20., 20. num1 = int(dur1 / bp.backend.get_dt()) num2 = int(dur2 / bp.backend.get_dt()) num3 = int(dur3 / bp.backend.get_dt()) position = np.zeros(num1 + num2 + num3) position[num1: num1 + num2] = np.linspace(0., 12., num2) position[num1 + num2:] = 12. position = position.reshape((-1, 1)) Iext = cann.get_stimulus_by_pos(position) cann.run(duration=dur1 + dur2 + dur3, inputs=('input', Iext)) plot_animate() "},"appendix.html":{"url":"appendix.html","title":"附录：模型代码","keywords":"","body":"附录 附录中所载代码为本书正文展示代码的完整版。我们亦提供常用模型的调用接口，如需查看，请参见github仓库https://github.com/PKU-NIP-Lab/BrainModels及文档https://brainmodels.readthedocs.io/en/latest/。 附录1. 神经元模型 附录2. 突触模型 附录3. 网络模型 "}}